<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>[Leetcode]Hot100-哈希</title>
    <link href="/2024/09/02/Leetcode-Hot100-%E5%93%88%E5%B8%8C/"/>
    <url>/2024/09/02/Leetcode-Hot100-%E5%93%88%E5%B8%8C/</url>
    
    <content type="html"><![CDATA[<h1 id="Leetcode-Hot100-哈希"><a href="#Leetcode-Hot100-哈希" class="headerlink" title="Leetcode Hot100 哈希"></a>Leetcode Hot100 哈希</h1><h2 id="1-两数之和"><a href="#1-两数之和" class="headerlink" title="1. 两数之和"></a>1. 两数之和</h2><p>题目描述：看<a href="https://leetcode.cn/problems/two-sum/?envType=study-plan-v2&envId=top-100-liked">这里</a></p><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><h4 id="暴力求解"><a href="#暴力求解" class="headerlink" title="暴力求解"></a>暴力求解</h4><p>看到这一题的时候首先想到的思路就是暴力解法, 在配置 C++ 项目环境的时候花了点功夫。</p><p>暴力求解的主要思路就是遍历数组, 两层循环遍历, 寻找两数之和为 target 的两个下标, 没什么好说的。时间复杂度应该是 O(N²)</p><p>正确答案：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">twoSum</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums, <span class="hljs-type">int</span> target)</span> </span>&#123;<br>        vector&lt;<span class="hljs-type">int</span>&gt; ans;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.<span class="hljs-built_in">size</span>(); i++)&#123;<br>                <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = i + <span class="hljs-number">1</span>; j &lt; nums.<span class="hljs-built_in">size</span>(); j++)&#123;<br>                    <span class="hljs-keyword">if</span>(nums[i] + nums[j] == target)&#123;<br>                        ans.<span class="hljs-built_in">push_back</span>(i);<br>                        ans.<span class="hljs-built_in">push_back</span>(j);<br>                        <span class="hljs-keyword">return</span> ans;<br>                    &#125;               <br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h4 id="排序加滑动窗口"><a href="#排序加滑动窗口" class="headerlink" title="排序加滑动窗口"></a>排序加滑动窗口</h4><p>这是我刚刚突然想到的思路, 如果先对给出的数组进行排序, 然后利用滑动窗口进行求解, 理论上也能做出来, 下次做这题的时候可以实践一下。</p><h4 id="哈希法"><a href="#哈希法" class="headerlink" title="哈希法"></a>哈希法</h4><p>主要思路是, 构造一个 map 主键为 num[i], 键值为i, 再遍历这个 map, 每遍历到一个, 查看是否存在能与其和为 target 的主键, 存在则 break 返回, 不存在则继续遍历。</p><p>正确答案：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">twoSum</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums, <span class="hljs-type">int</span> target)</span> </span>&#123;<br>        map&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt; a;<span class="hljs-comment">//建立hash表存放数组元素</span><br>        <span class="hljs-function">vector&lt;<span class="hljs-type">int</span>&gt; <span class="hljs-title">b</span><span class="hljs-params">(<span class="hljs-number">2</span>,<span class="hljs-number">-1</span>)</span></span>;<span class="hljs-comment">//存放结果</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;nums.<span class="hljs-built_in">size</span>();i++)<br>            a.<span class="hljs-built_in">insert</span>(map&lt;<span class="hljs-type">int</span>,<span class="hljs-type">int</span>&gt;::<span class="hljs-built_in">value_type</span>(nums[i],i));<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;nums.<span class="hljs-built_in">size</span>();i++)<br>        &#123;<span class="hljs-comment">// 这里要加不是本身的条件！</span><br>            <span class="hljs-keyword">if</span>(a.<span class="hljs-built_in">count</span>(target-nums[i])&gt;<span class="hljs-number">0</span>&amp;&amp;(a[target-nums[i]]!=i))<br>            <span class="hljs-comment">//判断是否找到目标元素且目标元素不能是本身</span><br>            &#123;<br>                b[<span class="hljs-number">0</span>]=i;<br>                b[<span class="hljs-number">1</span>]=a[target-nums[i]];<br>                <span class="hljs-keyword">break</span>;<br>            &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> b;<br>    &#125;;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="学会的-Trick"><a href="#学会的-Trick" class="headerlink" title="学会的 Trick"></a>学会的 Trick</h3><ul><li>nums.size() 返回 vector 容器的个数</li><li>当函数的返回值为 vector &lt; T &gt;的时候, 可以直接返回 {T, T}的数据类型</li><li>map.insert 插入键值对</li><li>map.count(val) 返回map中主键为 val 的个数, 没有则为 0</li><li>map[主键] 可以直接索引键值, 如果没有该主键则直接创建</li></ul><h2 id="49-字母异位词分组"><a href="#49-字母异位词分组" class="headerlink" title="49. 字母异位词分组"></a>49. 字母异位词分组</h2><p>题目描述：看<a href="https://leetcode.cn/problems/group-anagrams/description/?envType=study-plan-v2&envId=top-100-liked">这里</a></p><h3 id="题解-1"><a href="#题解-1" class="headerlink" title="题解"></a>题解</h3><p>典型的哈希法题目, 对 strs 里面的每个字符串进行 sort, 再构造 map 进行哈希索引, 主键为 sort 之后的字符串, 键值为 vector &lt; string &gt; , 构造完 map后, 遍历 map 把 map的每一个主键所包含的 vector 放到 vector &lt;  vector &lt; string &gt; &gt;  中。</p><p>正确答案：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    vector&lt;vector&lt;string&gt;&gt; <span class="hljs-built_in">groupAnagrams</span>(vector&lt;string&gt;&amp; strs) &#123;<br>        vector&lt;vector&lt;string&gt;&gt; ans;<br>        unordered_map&lt;string, vector&lt;string&gt;&gt; mp;<br><br>        <span class="hljs-keyword">for</span>(string &amp;str:strs)&#123;<br>            string temp = str;<br>            <span class="hljs-built_in">sort</span>(temp.<span class="hljs-built_in">begin</span>(), temp.<span class="hljs-built_in">end</span>());<br>            mp[temp].<span class="hljs-built_in">push_back</span>(str);<br>        &#125;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> it = mp.<span class="hljs-built_in">begin</span>(); it != mp.<span class="hljs-built_in">end</span>(); it++)<br>        &#123;<br>            ans.<span class="hljs-built_in">push_back</span>(it-&gt;second);<br>        &#125;<br>        <span class="hljs-keyword">return</span> ans;<br>        <br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="学会的-Trick-1"><a href="#学会的-Trick-1" class="headerlink" title="学会的 Trick"></a>学会的 Trick</h3><ul><li><p>遍历 vector 容器时, 如果只是单纯利用其值, 可以直接for(T &amp;str:strs){}</p></li><li><p>algorithm 中的 sort 非常好用, 常用来排序可随机访问迭代器的容器, 比如原生的 arr 和 vector 容器, 可以自定义排序, 默认是从小到大排序的, 具体可以看<a href="https://blog.csdn.net/qq_41575507/article/details/105936466">这里</a></p></li><li><p>unordered_map 和 map, map 底层实现是红黑树, 而 unordered_map 的底层实现是哈希表。 map 是有序的, unordered_map是无需的。map占用内存小, 查找为logn, unordered_map占用内存大, 查找为O(1)</p></li></ul><h2 id="128-最长连续序列"><a href="#128-最长连续序列" class="headerlink" title="128. 最长连续序列"></a>128. 最长连续序列</h2><p>题目描述, 看<a href="https://leetcode.cn/problems/longest-consecutive-sequence/description/?envType=study-plan-v2&envId=top-100-liked">这里</a></p><h3 id="题解-2"><a href="#题解-2" class="headerlink" title="题解"></a>题解</h3><h4 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h4><p>最开始看这道题时, 是用的滑动窗口的办法解决这个问题, 首先用 unordered_set 对 vector 的元素去重, 再将 unordered_set 中的元素拷贝到 vector 中进行sort排序,之后利用滑动窗口求解即可。</p><p>正确答案：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">longestConsecutive</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums)</span> </span>&#123;<br>        unordered_set&lt;<span class="hljs-type">int</span>&gt; uset;<br>        vector&lt;<span class="hljs-type">int</span>&gt; nums_set;<br>        <span class="hljs-comment">// 去重</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> &amp;num : nums)&#123;<br>            uset.<span class="hljs-built_in">insert</span>(num);<br>        &#125;<br>        <span class="hljs-comment">/*   直接这样写更简洁 vector&lt;int&gt; nums_set(uset.begin(), uset.end())  */</span>        <br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> it = uset.<span class="hljs-built_in">begin</span>(); it != uset.<span class="hljs-built_in">end</span>(); it++)<br>        &#123;<br>            nums_set.<span class="hljs-built_in">push_back</span>(*it);<br>        &#125;<br>        <span class="hljs-built_in">sort</span>(nums_set.<span class="hljs-built_in">begin</span>(), nums_set.<span class="hljs-built_in">end</span>());<br>        <span class="hljs-type">int</span> left = <span class="hljs-number">0</span>;<br>        <span class="hljs-type">int</span> right = <span class="hljs-number">1</span>;<br>        <span class="hljs-type">int</span> max_length = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">if</span>(nums_set.<span class="hljs-built_in">size</span>() != <span class="hljs-number">0</span>)<br>            max_length = <span class="hljs-number">1</span>;<br>        <span class="hljs-keyword">else</span><br>            max_length = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">while</span>(right &lt; nums_set.<span class="hljs-built_in">size</span>())&#123;<br>            <span class="hljs-type">int</span> length = <span class="hljs-number">1</span>;<br>            <span class="hljs-keyword">while</span>(right &lt; nums_set.<span class="hljs-built_in">size</span>() &amp;&amp; nums_set[right] == nums_set[left] + <span class="hljs-number">1</span> )&#123;<br>                length += <span class="hljs-number">1</span>;<br>                right += <span class="hljs-number">1</span>;<br>                left += <span class="hljs-number">1</span>;<br>            &#125;<br>            <span class="hljs-keyword">if</span>(length &gt; max_length)<br>                max_length = length;<br>            left = right ;<br>            right = right + <span class="hljs-number">1</span>;<br>        &#125;<br>        <span class="hljs-keyword">return</span> max_length;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h4 id="哈希法-1"><a href="#哈希法-1" class="headerlink" title="哈希法"></a>哈希法</h4><p>重要结论, “没有前驱的x一定是一段连续序列的开始”</p><p>正确答案:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">longestConsecutive</span><span class="hljs-params">(vector&lt;<span class="hljs-type">int</span>&gt;&amp; nums)</span> </span>&#123;<br>        unordered_set&lt;<span class="hljs-type">int</span>&gt; s;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> i : nums) s.<span class="hljs-built_in">insert</span>(i);<br>        <span class="hljs-type">int</span> ans = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> i : nums)<br>            <span class="hljs-keyword">if</span> (!s.<span class="hljs-built_in">count</span>(i - <span class="hljs-number">1</span>)) &#123;<br>                <span class="hljs-type">int</span> res = <span class="hljs-number">1</span>, cur = i;<br>                <span class="hljs-keyword">while</span> (s.<span class="hljs-built_in">count</span>(++ cur)) res ++;<br>                ans = <span class="hljs-built_in">max</span>(ans, res);<br>            &#125;<br>        <span class="hljs-keyword">return</span> ans;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="学会的-Trick-2"><a href="#学会的-Trick-2" class="headerlink" title="学会的 Trick"></a>学会的 Trick</h3><ul><li>元素去重+排序怎么实现</li><li>set&#x2F;unordered_set是元素唯一的容器</li><li>count在哈希法解题中的重要作用</li><li>滑动窗口解题时注意元素为0或1时的答案</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>算法题</tag>
      
      <tag>哈希</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>工作就业帖子</title>
    <link href="/2024/08/31/%E5%B7%A5%E4%BD%9C%E5%B0%B1%E4%B8%9A%E5%B8%96%E5%AD%90/"/>
    <url>/2024/08/31/%E5%B7%A5%E4%BD%9C%E5%B0%B1%E4%B8%9A%E5%B8%96%E5%AD%90/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>保研经验帖子</title>
    <link href="/2024/08/31/%E4%BF%9D%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%B8%96%E5%AD%90/"/>
    <url>/2024/08/31/%E4%BF%9D%E7%A0%94%E7%BB%8F%E9%AA%8C%E5%B8%96%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h2 id="保研经验帖存放"><a href="#保研经验帖存放" class="headerlink" title="保研经验帖存放"></a>保研经验帖存放</h2><p>现在是 24年8月31日, 我已经成为了大三的学生, 因为将来很可能获得保研资格, 因此打算从现在开始收集一些保研的经验帖子和信息, 供日后的自己使用。</p><h3 id="保研流程和时间线"><a href="#保研流程和时间线" class="headerlink" title="保研流程和时间线"></a>保研流程和时间线</h3><p>待补充……</p><h3 id="保研需要准备的东西"><a href="#保研需要准备的东西" class="headerlink" title="保研需要准备的东西"></a>保研需要准备的东西</h3><ul><li>自己的简历(建议找学长学姐多修改修改、看看优秀的学长学姐的简历)</li><li>英语, 最起码能够完成一个 3-5 min 的纯英文自我介绍</li><li>最好具备几个 solid 的项目, 并且知道面试的时候老师会问什么</li><li>Acwing 和 Leetcode 要坚持刷</li></ul><h3 id="获取信息的一些渠道"><a href="#获取信息的一些渠道" class="headerlink" title="获取信息的一些渠道"></a>获取信息的一些渠道</h3><ul><li>CS-BAOYAN的 <a href="https://github.com/CS-BAOYAN">github</a></li><li>绿群, 在上面的那个 github 中提到了</li><li>上面的 CS-BAOYAN 的资源很多, 最好多看看多了解了解</li><li>计算机保研文书<a href="https://github.com/yuezih/King-of-Pigeon">实用模板</a></li></ul><h3 id="目标院校"><a href="#目标院校" class="headerlink" title="目标院校"></a>目标院校</h3><ul><li>人大高瓴、信息学院</li><li>北航计算机学院</li><li>中科院计算所</li><li>北理工</li><li>清深(梦校)</li><li>北深(梦校)</li></ul><h3 id="经验贴指路"><a href="#经验贴指路" class="headerlink" title="经验贴指路"></a>经验贴指路</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/659337046?utm_campaign=&utm_medium=social&utm_psn=1812904731204530176&utm_source=qq">人大高瓴、清深、北深</a> bg: 西南某9 rank 5%, 弱竞赛, 两段科研经历 上岸人大高瓴直博</li><li></li></ul>]]></content>
    
    
    <categories>
      
      <category>经验</category>
      
    </categories>
    
    
    <tags>
      
      <tag>保研</tag>
      
      <tag>经验</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>思归笛,竹静影</title>
    <link href="/2024/08/28/%E6%80%9D%E5%BD%92%E7%AC%9B_%E7%AB%B9%E9%9D%99%E5%BD%B1/"/>
    <url>/2024/08/28/%E6%80%9D%E5%BD%92%E7%AC%9B_%E7%AB%B9%E9%9D%99%E5%BD%B1/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>琐事</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>换硬盘+重装系统历险记</title>
    <link href="/2024/07/27/%E6%8D%A2%E7%A1%AC%E7%9B%98-%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%8E%86%E9%99%A9%E8%AE%B0/"/>
    <url>/2024/07/27/%E6%8D%A2%E7%A1%AC%E7%9B%98-%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%8E%86%E9%99%A9%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="换硬盘-重装系统历险记"><a href="#换硬盘-重装系统历险记" class="headerlink" title="换硬盘+重装系统历险记"></a>换硬盘+重装系统历险记</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>​在学校的时候由于学长的馈赠偶然得到一块2TB的固态硬盘, 暑假集训完回到家后闲的没事, 刚好也把这块硬盘带来了, 就想着装上。我这款电脑是 dell g15 5520 的 2022 款, 难绷的是他的硬盘槽只有一个卡位, 因此只能把原来的硬盘拆下来换上新的。硬盘在京东的介绍是<u>金士顿(Kingston) 2TB SSD固态硬盘M.2(NVMe PCIe 4.0×4)兼容PCIe3.0 NV2 读速3500MB&#x2F;s AI 电脑配件</u>。</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><h4 id="数据备份"><a href="#数据备份" class="headerlink" title="数据备份"></a>数据备份</h4><p>​现在不是简单的加装硬盘了, 而是要把硬盘换掉。因此需要重新装 windows 系统, 首先把重要数据备份一下, 这里难绷的又出现了, 我就一个u盘但是这个盘还得刻录成系统盘, 而且一开始我并不知道系统盘要先全部格式化….. Anyway, 最后我是把重要的文件压缩之后发送给 QQ 的自己了, QQ 有个限制, 就是单个文件传输不能超过4GB, 文件传输超过 1GB 保存时间只有7天, 我有的压缩包确实超过了4GB, 我就把各个子文件夹分别压缩发送了, 对我来说 7 天的保存时间足够换硬盘了。</p><h4 id="制作系统盘"><a href="#制作系统盘" class="headerlink" title="制作系统盘"></a>制作系统盘</h4><p>​备份完数据后, 就开始要刻录系统盘了, 主要步骤就是去微软官网下载安装包, 然后跟着安装包一路安装就行。具体教程可以参考<a href="https://blog.csdn.net/qq_46034913/article/details/131707076">这里</a>。但是这里制作系统盘的时候有点坑,在安装windows系统的时候会报出<code>Windows无法安装所需的文件。请确保安装所需的所有文件可用，并重新启动安装</code>, 我现在不清楚是哪种原因引起的, 可疑的原因有两个, 一是选择window安装包的时候选的是win11家庭版但是这个家庭版有点bug, 应该直接选win11; 二是我的u盘文件系统没有设置为NTFS类型, 在创建启动盘前将其设置为NTFS系统, 创建完成后又变成了 FAT32系统, 因此需要再更改一遍。解决方法看<a href="https://blog.csdn.net/shaun2001/article/details/136293625">这里</a>, 具体步骤就是启动盘创建完毕后再改一次格式, 然后下载镜像文件解压到u盘, 原因我也是在那篇文章的评论区里面找到的。</p><h3 id="换硬盘"><a href="#换硬盘" class="headerlink" title="换硬盘"></a>换硬盘</h3><p>​首先是拆解螺丝, 拆解螺丝的顺序如下图所示。注意只有标号为1、2、3、4的螺丝可以拆下来, 5、6、7、8只能拧松不能拆卸。</p><p><img src="/images/post_5/dell_chai.png" alt="螺丝拆解顺序"></p><p>​另外这些螺丝都是 M2 螺丝, 最好找个合适的工具拆卸, <strong>不然会将螺丝拧成圆口导致无法拆卸</strong>(血与泪的教训啊！！！), 拧回去的时候也<strong>不要拧得太紧</strong>！</p><p>​拆完后壳(后来了解到那个壳叫D壳)后, 将硬盘槽原来的硬盘拆下来, 找到硬盘槽位置(如下图), 并且将固定装置移到最边上, 具体教程看<a href="https://www.bilibili.com/video/BV1pL411x7WW/?buvid=XY9604D9E1B7EE25A631B0C948D08D829ADCB&from_spmid=search.search-result.0.0&is_story_h5=false&mid=DrENeUX2dmjk3L1W1TBVKg==&p=1&plat_id=114&share_from=ugc&share_medium=android&share_plat=android&share_session_id=229e4d56-e1d3-422e-9df1-1c8445981789&share_source=QQ&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1722071439&unique_k=wOYASGI&up_id=15680833">这里</a>,拆这个移动装置可能费些力气, 之后就安装自己的硬盘槽就好了！</p><p><img src="/images/post_5/dell_disk.jpg" alt="硬盘槽位置"></p><p>​关于换硬盘, 听说别的牌子的笔记本要拔电(我指的是电池对主板的供电,拆了后盖才能看见), 但是据说dell 拔电装硬盘会导致静电累积, 不用拔电直接换硬盘就行, 我没有拔电, 直接换的硬盘, 目前看来还挺好的, 没什么大毛病。</p><h3 id="装系统"><a href="#装系统" class="headerlink" title="装系统"></a>装系统</h3><p>​在装系统之前需要知道的是, dell 的 BIOS menu 是 F2, BIOS setup 是 F12。</p><h4 id="windows"><a href="#windows" class="headerlink" title="windows"></a>windows</h4><p>​ 正常情况下重装系统只需要进入 BIOS setup 选择启动盘进入安装系统引导就行, 但是这一次是换装硬盘之后重新装系统, 如果直接进入启动盘装 window 会出现下面的情况, 找不到硬盘。</p><p><img src="/images/post_5/disknotfind.png" alt="无法找到硬盘"></p><p>​解决方法看<a href="https://www.bilibili.com/video/BV1em4y1j7fR/?buvid=XY9604D9E1B7EE25A631B0C948D08D829ADCB&from_spmid=main.my-history.0.0&is_story_h5=false&mid=DrENeUX2dmjk3L1W1TBVKg==&p=1&plat_id=114&share_from=ugc&share_medium=android&share_plat=android&share_session_id=1414ce1f-581d-4e73-a327-44b442fc1cf1&share_source=QQ&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1722072183&unique_k=sYXtCTX&up_id=1369219423">这里</a>, 具体解决步骤是 F2进入BIOS menu -&gt; storage -&gt; 勾选 AHCI&#x2F;NVMe -&gt; 弹出的对话框勾选再OK -&gt; apply changes</p><p>​这样window安装时就能扫到新换的硬盘了。</p><h5 id="windows-装office-全家桶"><a href="#windows-装office-全家桶" class="headerlink" title="windows 装office 全家桶"></a>windows 装office 全家桶</h5><p>​看<a href="https://www.bilibili.com/video/BV1RH4y1B7GM/?spm_id_from=333.788.recommend_more_video.-1&vd_source=0dfb93df2219f50d2546566e4abe75a7">这里</a>, 安装 office tool plus 然后 kms 激活。</p><h4 id="ubuntu"><a href="#ubuntu" class="headerlink" title="ubuntu"></a>ubuntu</h4><p>​安装完 windows 之后就要安装 ubuntu, 需要重新制作一个启动盘, 记得提前分出盘来给 ubuntu 使用, 制作好启动盘后其他的流程跟装 windows 一样, 注意分区的选取, 在 Installation type 界面选择 something else, 这样就自定义分区了。关于安装 ubuntu 系统, <a href="https://blog.csdn.net/qq_42313591/article/details/136007211">这篇文章</a>讲的特别好, 少有的从准备阶段到最后的注意事项都讲到了, 而且还会讲每一步这样做的原因, 强烈推荐！</p><h5 id="ubuntu-显卡驱动安装"><a href="#ubuntu-显卡驱动安装" class="headerlink" title="ubuntu 显卡驱动安装"></a>ubuntu 显卡驱动安装</h5><p>​安装完 ubuntu 后还要安装显卡驱动以适应后面的需要, 关于显卡驱动的安装, 个人认为最稳妥的方式还是在官网下载 .run 文件后再在本地安装, 安装教程可以看<a href="https://blog.csdn.net/iiloveChina/article/details/139436321">这篇</a>, 在安装的时候推荐在命令后面添加 -no-opengl-files, 禁用 opengl, 这样防止安装完成后造成循环登录的局面。</p><p>​不幸运的是, 在装完显卡驱动后, 重启进入系统一直卡在  <code>/dev/xxxxx clean: xxxx blocks</code>, 这个界面。网上找了很多方法, <a href="https://www.bilibili.com/read/cv28673068/">这篇文章</a>的方法是有效的, 具体原理 GPT 是这样写的: <code>nomodeset</code>参数禁用了内核自动设置图形模式的功能，通常用于解决与图形驱动相关的兼容性问题，比如显卡驱动不兼容或错误配置导致的启动问题。</p><p>​为了防止作者把文章删了, 偷偷在这里做一下备份。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">1、开机按住<span class="hljs-built_in">shift</span>键，直到出现选择Ubuntu启动方式的时候，按E进入Grub模式<br><br>2、在Grub模式下，倒数第二行有ro的地方，修改为rw quiet splash nomodeset <span class="hljs-variable">$vt_handoff</span><br><br>3、按F10进入系统<br><br>4.、进去系统之后编辑grub文件  <span class="hljs-built_in">sudo</span> vim /etc/default/grub<br><br>5、找到这一行:<br><br>      GRUB_CMDLINE_LINUX_DEFAULT=<span class="hljs-string">&quot;quiet splash&quot;</span><br><br>修改为：<br><br>      GRUB_CMDLINE_LINUX_DEFAULT=<span class="hljs-string">&quot;quiet splash nomodeset&quot;</span><br><br>6.、更新GRUB： <span class="hljs-built_in">sudo</span> update-grub<br><br>7、保存，并重新开机。 <br></code></pre></td></tr></table></figure><p>​解决了显卡驱动问题后, 后面就是安装 cuda 和 cudnn 了, 应该都不是什么大问题。</p><h5 id="ubuntu-安装搜狗输入法"><a href="#ubuntu-安装搜狗输入法" class="headerlink" title="ubuntu 安装搜狗输入法"></a>ubuntu 安装搜狗输入法</h5><p>​重装系统后, 系统自带的拼音实在难评, 因此需要装一下搜狗输入法, 我在安装系统时就已经选择了中文, 这极大方便了我系统上搜狗输入法的安装, 具体安装步骤可以参考<a href="https://blog.csdn.net/qq_44684757/article/details/135991216#_12">这篇文章</a>, 写得很详细, 照着做就可以了。</p><h5 id="ubuntu-迁移博客网站"><a href="#ubuntu-迁移博客网站" class="headerlink" title="ubuntu 迁移博客网站"></a>ubuntu 迁移博客网站</h5><p>​重装系统后, 原来写博客的文件夹也做了备份, 但是不知道怎么迁移, 网上也没有教程, 自己的思路就是安装完相关依赖之后直接在文件夹下运用命令行。我的做法是这样的。</p><ul><li><p>首先安装一下 git, 很简单, <a href="https://cn.linux-console.net/?p=29897">这篇文章</a>照着做就行, 直接 apt install 然后配置一下邮箱和用户名即可。</p></li><li><p>安装 nodejs 和 npm, <a href="https://blog.csdn.net/weixin_55719805/article/details/128094550">安装方法</a>推荐第一种源码安装, 因为已经在第二种踩过坑了, 这种没有配置环境变量, 还得自己配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">npm root -g 查看全局包的路径</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">第二种方法安装完成后, 需要在 bashrc 里面写入</span><br>export PATH=$PATH:/home/sigmoid/nodejs/lib/node_modules/npm/bin<br></code></pre></td></tr></table></figure></li><li><p>安装 hexo-cli, <code>npm install hexo-cli -g</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">安装完 hexo-cli 后,还需配置hexo环境变量, 写入</span><br>export PATH=$PATH:/home/sigmoid/nodejs/lib/node_modules/hexo-cli/bin<br></code></pre></td></tr></table></figure></li><li><p>进入到 hexo-blog 目录下 <code>npm install</code></p></li><li><p>运行 <code>hexo generate</code> 和 <code>hexo server</code> 看是否能运行</p></li><li><p>最后配置一下 github 的 ssh 密钥, 教程看<a href="https://blog.csdn.net/weixin_42569846/article/details/105808683">这里</a>, 就可以使用 <code>hexo g -d</code> 命令远程部署了。</p></li></ul><h5 id="ubuntu-安装-cuda-和-cudnn"><a href="#ubuntu-安装-cuda-和-cudnn" class="headerlink" title="ubuntu 安装 cuda 和 cudnn"></a>ubuntu 安装 cuda 和 cudnn</h5><p>​安装 cuda 看<a href="https://blog.csdn.net/h3c4lenovo/article/details/119003405">这篇</a>, 主要是记住这个 cuda 的安装包的<a href="https://developer.nvidia.com/cuda-toolkit-archive">下载网址</a>。</p><p>​安装 cudnn 看<a href="https://blog.csdn.net/qq_32033383/article/details/135015041">这篇</a>, 讲的 deb 安装方法, 很详细, 现在新版的 cudnn 都是 deb 安装了, 不会再有压缩包的那种安装方式了。</p><h5 id="ubuntu-创建快捷方式"><a href="#ubuntu-创建快捷方式" class="headerlink" title="ubuntu 创建快捷方式"></a>ubuntu 创建快捷方式</h5><p>​看<a href="https://blog.csdn.net/zi_longh/article/details/123968434">这里</a>, 基本思路就是创建 .desktop 文件。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​写到这里, 换硬盘+重装双系统的过程大概也记录得差不多了, 剩下的就是一些开发工具的安装和配置。这次经历有收获也有教训, 收获是自己体验了一把拆卸电脑换硬盘和装系统的乐趣, 在这之前我是没有自主装过系统的, 更别说一下装两个了, 还挺有趣, 一些问题网上都有解决办法, 自己的实操能力又得到了加强。教训是应该充分地做好准备工作, 万万没想到最遗憾的竟然是拆卸环节, 螺丝拧圆了, 导致我大力出奇迹把D壳损坏了, 好在伤势不严重, 能接着用, 也看不出啥区别, 就是少了一个螺丝固定处, 以后一定得买一个趁手的工具再拆卸螺丝, 所谓工欲善其事必先利其器。</p><p>​马上要到七月底了, 回到家里也快一周了, 是时候该找同学朋友们聚一聚了。</p><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><ol><li><a href="https://blog.csdn.net/qq_46034913/article/details/131707076">https://blog.csdn.net/qq_46034913/article/details/131707076</a></li><li><a href="https://blog.csdn.net/shaun2001/article/details/136293625">https://blog.csdn.net/shaun2001/article/details/136293625</a></li><li><a href="https://www.bilibili.com/video/BV1pL411x7WW/?buvid=XY9604D9E1B7EE25A631B0C948D08D829ADCB&from_spmid=search.search-result.0.0&is_story_h5=false&mid=DrENeUX2dmjk3L1W1TBVKg==&p=1&plat_id=114&share_from=ugc&share_medium=android&share_plat=android&share_session_id=229e4d56-e1d3-422e-9df1-1c8445981789&share_source=QQ&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1722071439&unique_k=wOYASGI&up_id=15680833">https://www.bilibili.com/video/BV1pL411x7WW/?buvid=XY9604D9E1B7EE25A631B0C948D08D829ADCB&amp;from_spmid=search.search-result.0.0&amp;is_story_h5=false&amp;mid=DrENeUX2dmjk3L1W1TBVKg%3D%3D&amp;p=1&amp;plat_id=114&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=229e4d56-e1d3-422e-9df1-1c8445981789&amp;share_source=QQ&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1722071439&amp;unique_k=wOYASGI&amp;up_id=15680833</a></li><li><a href="https://www.bilibili.com/video/BV1em4y1j7fR/?buvid=XY9604D9E1B7EE25A631B0C948D08D829ADCB&from_spmid=main.my-history.0.0&is_story_h5=false&mid=DrENeUX2dmjk3L1W1TBVKg==&p=1&plat_id=114&share_from=ugc&share_medium=android&share_plat=android&share_session_id=1414ce1f-581d-4e73-a327-44b442fc1cf1&share_source=QQ&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1722072183&unique_k=sYXtCTX&up_id=1369219423">https://www.bilibili.com/video/BV1em4y1j7fR/?buvid=XY9604D9E1B7EE25A631B0C948D08D829ADCB&amp;from_spmid=main.my-history.0.0&amp;is_story_h5=false&amp;mid=DrENeUX2dmjk3L1W1TBVKg%3D%3D&amp;p=1&amp;plat_id=114&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=1414ce1f-581d-4e73-a327-44b442fc1cf1&amp;share_source=QQ&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1722072183&amp;unique_k=sYXtCTX&amp;up_id=1369219423</a></li><li><a href="https://blog.csdn.net/qq_42313591/article/details/136007211">https://blog.csdn.net/qq_42313591/article/details/136007211</a></li><li><a href="https://blog.csdn.net/iiloveChina/article/details/139436321">https://blog.csdn.net/iiloveChina/article/details/139436321</a></li><li><a href="https://www.bilibili.com/read/cv28673068/">https://www.bilibili.com/read/cv28673068/</a></li><li><a href="https://blog.csdn.net/qq_44684757/article/details/135991216#_12">https://blog.csdn.net/qq_44684757/article/details/135991216#_12</a></li><li><a href="https://cn.linux-console.net/?p=29897">https://cn.linux-console.net/?p=29897</a></li><li><a href="https://blog.csdn.net/weixin_55719805/article/details/128094550">https://blog.csdn.net/weixin_55719805/article/details/128094550</a></li><li><a href="https://blog.csdn.net/weixin_42569846/article/details/105808683">https://blog.csdn.net/weixin_42569846/article/details/105808683</a></li><li><a href="https://blog.csdn.net/h3c4lenovo/article/details/119003405">https://blog.csdn.net/h3c4lenovo/article/details/119003405</a></li><li><a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a></li><li><a href="https://blog.csdn.net/qq_32033383/article/details/135015041">https://blog.csdn.net/qq_32033383/article/details/135015041</a></li><li><a href="https://blog.csdn.net/zi_longh/article/details/123968434">https://blog.csdn.net/zi_longh/article/details/123968434</a></li><li><a href="https://www.bilibili.com/video/BV1RH4y1B7GM/?spm_id_from=333.788.recommend_more_video.-1&vd_source=0dfb93df2219f50d2546566e4abe75a7">https://www.bilibili.com/video/BV1RH4y1B7GM/?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=0dfb93df2219f50d2546566e4abe75a7</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>硬盘</tag>
      
      <tag>重装系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLO的数据增强与参数调整</title>
    <link href="/2024/07/16/YOLO%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/"/>
    <url>/2024/07/16/YOLO%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/</url>
    
    <content type="html"><![CDATA[<h2 id="YOLO的数据增强与参数调整"><a href="#YOLO的数据增强与参数调整" class="headerlink" title="YOLO的数据增强与参数调整"></a>YOLO的数据增强与参数调整</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>​被某比赛的视觉任务反复折磨之后, 我发现 yolo 的泛化性能不仅在目标检测领域, 甚至在图像分类领域都有着很高的地位。</p><p>​事情是这样的, 我拿 resnet 的各个变种去训练 cifar100 , 训练出的模型泛化性能极其不行, 经常过拟合; 我拿cifar100 benchmark上面的sota们(State of the arts, 指的是在某一个领域做的Performance最好的model)去训练发现他们大部分是为了刷sota而做的模型, 他们模型的预测结果虽然是正确的但是置信度极低(0.1x), 拿训练好的模型去预测效果也很差。但是如果同样的输入放到 yolo 模型里面得到的输出正确率竟然高的惊人。</p><p>​说明一下前提, 我的输入仍然是训练集的东西, 但是由于实际环境的影响跟原来的训练集可能在饱和度、色调、亮度等方面都有不同。</p><p>​曾经用 cifar100 训练 resnet18&#x2F;34 的时候, 模型的过拟合现象严重, 我也曾经试着将预处理加上亮度、色调、饱和度的随机变化, 但是效果都很一般。</p><p>​因此,  我很好奇 YOLO 在数据增强阶段应用了什么样的操作, 另外, 在 yolo 的训练结果的 weight 中有 best_weights 和 last_weights 这两个权重, 我也想知道它评判每一轮的权重是否为 best 的依据是什么(？是不是只是简单的评价一下哪一轮的测试集正确率最高哪个就是best)</p><div class="note note-info">            <p>​2024.7.16: 再挖个坑, 等自己填, 感觉要等一段时间, 最近有点忙, 估计没工夫去看 yolo 源码, 或许可以直接GPT, 但是最好还是自己在源码里面找到这部分逻辑, 毕竟眼见为实。等填坑的时候一定要 callback 一下这里！</p>          </div><h3 id="数据增强操作"><a href="#数据增强操作" class="headerlink" title="数据增强操作"></a>数据增强操作</h3><p>​<strong>YOLO的目标检测训练的数据增强和分类的数据增强是不同的。</strong></p><p>​yolov8的数据增强操作是在<code>ultralytics-main/ultralytics/data/augment.py</code>的 <code>v8_transforms</code>里面, 具体的代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">v8_transforms</span>(<span class="hljs-params">dataset, imgsz, hyp, stretch=<span class="hljs-literal">False</span></span>):<br>    pre_transform = Compose(<br>            [<br>                Mosaic(dataset, imgsz=imgsz, p=hyp.mosaic),<br>                CopyPaste(p=hyp.copy_paste),<br>                RandomPerspective(<br>                    degrees=hyp.degrees,<br>                    translate=hyp.translate,<br>                    scale=hyp.scale,<br>                    shear=hyp.shear,<br>                    perspective=hyp.perspective,<br>                    pre_transform=<span class="hljs-literal">None</span> <span class="hljs-keyword">if</span> stretch <span class="hljs-keyword">else</span> LetterBox(new_shape=(imgsz, imgsz)),<br>                ),<br>            ]<br>        )<br>    <span class="hljs-keyword">return</span> Compose(<br>            [<br>                pre_transform,<br>                MixUp(dataset, pre_transform=pre_transform, p=hyp.mixup),<br>                Albumentations(p=<span class="hljs-number">1.0</span>),<br>                RandomHSV(hgain=hyp.hsv_h, sgain=hyp.hsv_s, vgain=hyp.hsv_v),<br>                RandomFlip(direction=<span class="hljs-string">&quot;vertical&quot;</span>, p=hyp.flipud),<br>                RandomFlip(direction=<span class="hljs-string">&quot;horizontal&quot;</span>, p=hyp.fliplr, flip_idx=flip_idx),<br>            ]<br>        )  <span class="hljs-comment"># transforms</span><br></code></pre></td></tr></table></figure><p>​本来以为跟我训resnet一样的流程, transform就是常见的那几个然后进行排列组合调整调整参数, 看了源码以后我才发现我有点太天真了, 感觉这个数据增强是成体系的。下面我一行一行来剖析这个数据增强。</p><ul><li>Mosaic</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>cv</tag>
      
      <tag>目标检测</tag>
      
      <tag>yolo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]pytorch模型训练pipline总结</title>
    <link href="/2024/07/16/DL-pytorch%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83pipline%E6%80%BB%E7%BB%93/"/>
    <url>/2024/07/16/DL-pytorch%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83pipline%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h2 id="DL-pytorch模型训练pipline总结"><a href="#DL-pytorch模型训练pipline总结" class="headerlink" title="[DL]pytorch模型训练pipline总结"></a>[DL]pytorch模型训练pipline总结</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>​在前面的<a href="https://sigmoidsee.github.io/2024/07/16/DL-%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%E5%AE%9E%E6%88%98/">LSTM实现文本情绪识别的博客</a>, 还有训练流程这一 part没写, 打算单独写个博客来总结一下 pytorch 模型训练的 pipline, 于是有了这一篇博客。</p><h3 id="训练pipline"><a href="#训练pipline" class="headerlink" title="训练pipline"></a>训练pipline</h3><p>​下面是一般训练模型的流程, 具体的任务可能有些细节上的差异, 但是总体的流程都是大差不差的。</p><h4 id="定义数据加载器"><a href="#定义数据加载器" class="headerlink" title="定义数据加载器"></a>定义数据加载器</h4><p>​这一步准备方式有多种, NLP与CV可能不尽相同, 可能有的NLP任务不需要transform预处理, 直接用<code>TensorDataset</code>来代替。具体任务具体分析, 但是总归要获得一个带着batch的DataLoader。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="hljs-number">0.5</span>,), (<span class="hljs-number">0.5</span>,))])<br>train_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>, transform=transform)<br>train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h4 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h4><p>​需要说明的一点是不同的任务所需要的损失函数和优化器可能不尽相同, 在定义自己任务所需要的损失函数和优化器之前, 最好去网上做做功课然后定义, 可以尝试多种组合的损失函数和优化器来对比分析一下哪种组合的损失函数和优化器更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># loss and optimization functions</span><br>lr=<span class="hljs-number">0.001</span><br>criterion = nn.BCELoss() <span class="hljs-comment"># 常用于二分类的损失函数</span><br>optimizer = torch.optim.Adam(net.parameters(), lr=lr)<br></code></pre></td></tr></table></figure><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p>​一般为了防止爆了显存, 一个 epoch 往往分成多个 batch 来计算, 这部分训练的过程往往放在一个 for 循环里面。</p><ul><li><p>定义 epoch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs = <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure></li><li><p>开启两轮循环, 第一个 for 循环是为了训练完 <code>num_epochs</code>轮训练集, 第二个 for 循环是为了训练完一个 <code>epoch</code>里面的 <code>batches</code>, 每一个 epoch 循环完都要清空 loss。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">for</span> epoch in range(num_epochs):<br>    <span class="hljs-attribute">running_loss</span> = <span class="hljs-number">0</span>.<span class="hljs-number">0</span><br>    <span class="hljs-attribute">for</span> inputs, labels in train_loader:<br></code></pre></td></tr></table></figure></li><li><p>将输入和标签传送到GPU (如果可用)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs, labels = inputs.to(device), labels.to(device)<br></code></pre></td></tr></table></figure></li><li><p>将优化器梯度清零</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer.zero_grad()<br></code></pre></td></tr></table></figure></li><li><p>前向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">outputs = model(inputs)<br></code></pre></td></tr></table></figure></li><li><p>计算损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">loss = criterion(outputs, labels)<br></code></pre></td></tr></table></figure></li><li><p>反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">loss.backward()<br></code></pre></td></tr></table></figure></li><li><p>优化器进行参数更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer.step()<br></code></pre></td></tr></table></figure></li><li><p>累计损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">running_loss += loss.item()<br></code></pre></td></tr></table></figure></li><li><p>打印结果(二轮循环外)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Epoch [<span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>], Loss: <span class="hljs-subst">&#123;running_loss / <span class="hljs-built_in">len</span>(train_loader)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="评估模型-可选"><a href="#评估模型-可选" class="headerlink" title="评估模型(可选)"></a>评估模型(可选)</h3><p>​在实际过程中为了评估模型的泛化性能, 在训练完一轮训练数据或者循环完整个epochs之后, 往往都还会设置评估模型这一步, 下面说明一下评估模型的pipline。</p><ul><li><p>定义数据加载器(同上)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 准备测试数据集和数据加载器</span><br>test_dataset = datasets.MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">False</span>, download=<span class="hljs-literal">True</span>, transform=transform)<br>test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></li><li><p>切换模型到评估模式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 将模型设置为评估模式。评估模式下，某些层（如Dropout、BatchNorm）会有不同的行为。</span><br></code></pre></td></tr></table></figure></li><li><p>初始化变量以记录正确预测的数量和总样本数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">correct = <span class="hljs-number">0</span>  <span class="hljs-comment"># 用于存储正确预测的样本数量</span><br>total = <span class="hljs-number">0</span>  <span class="hljs-comment"># 用于存储测试样本的总数量</span><br></code></pre></td></tr></table></figure></li><li><p>开始评估</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 禁用梯度计算以加快评估速度</span><br><span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 禁用梯度计算，因为在评估过程中不需要反向传播</span><br>    <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> test_loader:  <span class="hljs-comment"># 遍历测试数据集中的所有样本</span><br>        inputs, labels = inputs.to(device), labels.to(device)  <span class="hljs-comment"># 将数据移动到GPU（如果可用）</span><br>        outputs = model(inputs)  <span class="hljs-comment"># 获取模型的预测输出</span><br>        _, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 获取每个输入样本的预测标签</span><br>        total += labels.size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 更新总样本数</span><br>        correct += (predicted == labels).<span class="hljs-built_in">sum</span>().item()  <span class="hljs-comment"># 更新正确预测的样本数量</span><br></code></pre></td></tr></table></figure></li><li><p>计算并打印模型在测试集上的准确率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy = <span class="hljs-number">100</span> * correct / total  <span class="hljs-comment"># 计算准确率</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Accuracy of the model on the test images: <span class="hljs-subst">&#123;accuracy&#125;</span> %&#x27;</span>)  <span class="hljs-comment"># 打印准确率</span><br></code></pre></td></tr></table></figure></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​本文总结了 pytorch 模型训练的一般流程, 前提是假定你已经定义好自己的模型并且完成了实例化的。需要注意的是具体的任务场景下可能还需要额外的步骤, 但是大致都逃不了上面的这些步骤。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]基于LSTM的文本情绪识别实战</title>
    <link href="/2024/07/16/DL-%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%E5%AE%9E%E6%88%98/"/>
    <url>/2024/07/16/DL-%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84%E6%96%87%E6%9C%AC%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<h2 id="基于LSTM的文本情绪识别实战"><a href="#基于LSTM的文本情绪识别实战" class="headerlink" title="基于LSTM的文本情绪识别实战"></a>基于LSTM的文本情绪识别实战</h2><p>​本项目基于LSTM实现, 应用的是 pytorch 深度学习框架。项目源码<a href="https://github.com/sigmoidsee/LSTMbyPytorch">这里</a>，因为我这是私密的库，如果将来有一天我的博客网站被你发现了，恰巧你又对这个项目感兴趣, 可以联系我给你放开权限hh.</p><h3 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h3><p>​能根据一段评价来预测这段评价是积极的还是消极的, 类似于二分器但又不能完全用传统的二分器来实现, 因为语言还有语序的因素。</p><p><img src="/images/post_4/1.png" alt="实现效果"></p><h3 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h3><p>​感觉大部分 NLP 任务的 pipline 都大差不差, 基本都要经历 数据清洗 -&gt; 数据集的划分 -&gt; 定义模型 -&gt; 实例模型 -&gt; 模型训练 -&gt; 模型评估 这几部分, 关于word2vec,这次任务是集成在定义模型里了, 我了解还不是很多, 目前只知道 one-hot 这种最简单的编码方式, 后面了解多了会专门写一篇博客来专门聊这个问题。</p><h4 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h4><p>​训练数据集的清洗主要包括去除训练数据中的特殊符号(!”#$%&amp;’()*+,-.&#x2F;:;&lt;&#x3D;&gt;?@[]^_&#96;{|}~)、去除换行符、去除空格、word2int映射、剪枝与填充、标签数据的清洗这几个操作。</p><ul><li><p>去除训练数据中的特殊符号</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> string <span class="hljs-keyword">import</span> punctuation<br><br><span class="hljs-built_in">print</span>(punctuation)<br><br><span class="hljs-comment"># get rid of punctuation</span><br>reviews = reviews.lower() <span class="hljs-comment"># lowercase, standardize</span><br>all_text = <span class="hljs-string">&#x27;&#x27;</span>.join([c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> reviews <span class="hljs-keyword">if</span> c <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> punctuation])<br></code></pre></td></tr></table></figure></li><li><p>去除训练数据中的换行符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">这里的数据清洗一定要核对一下长度,有时候明明文本文件没有换行符,但是读取是在最后多了一个换行符</span><br><span class="hljs-string">导致长度+1</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>reviews_split = all_text.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>reviews_split = [review <span class="hljs-keyword">for</span> review <span class="hljs-keyword">in</span> reviews_split <span class="hljs-keyword">if</span> review] <span class="hljs-comment"># 去除空字符串</span><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(reviews_split))<br>all_text = <span class="hljs-string">&#x27; &#x27;</span>.join(reviews_split)<br></code></pre></td></tr></table></figure></li><li><p>去除训练数据中的空格</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># create a list of words  # 去除空格</span><br>words = all_text.split()<br></code></pre></td></tr></table></figure></li></ul><p>​前面两步去除完之后得到的<code>reviews</code>、<code>reviews_split</code>都是分割后形成的列表, 为了方便后面的数据清洗操作, 一般都要再合成字符串, 所以要进行<code>&#39; &#39;.join(reviews_split)</code>这样的操作。</p><p>​合理的运用<strong>列表推导式</strong>会使得你的代码显得简洁易懂。</p><ul><li><p>word2int 映射</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><br><span class="hljs-comment"># 将每个单词映射到一个唯一的整数 ID 下面三行代码是按照单词出现的次序从高到低给单词一个整数值的映射</span><br><span class="hljs-comment"># 比如单词 &#x27;me&#x27; 出现的最多那么它的整数值映射就是1</span><br>counts = Counter(words)<br>vocab = <span class="hljs-built_in">sorted</span>(counts, key=counts.get, reverse=<span class="hljs-literal">True</span>)<br>vocab_to_int = &#123;word: ii <span class="hljs-keyword">for</span> ii, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocab, <span class="hljs-number">1</span>)&#125; <span class="hljs-comment"># 这里是从1开始映射的</span><br><br><span class="hljs-comment"># 这是一个二维列表, 每一行存储的是一条review的整数映射</span><br>reviews_ints = []<br><span class="hljs-keyword">for</span> review <span class="hljs-keyword">in</span> reviews_split:<br>    reviews_ints.append([vocab_to_int[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> review.split()])<br></code></pre></td></tr></table></figure></li><li><p>剪枝与填充</p><p>理想情况下我们输入的训练数据都是等长的, 这样能极大的简化模型的定义, 为了实现训练数据等长, 我们需要</p><p>训练数据进行剪枝与填充, 我们需要规定一个共用的长度, 当训练数据长度大于该标准长度时, 需要进行剪枝, 反之, 则需要进行填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">-len(row) 是一个负索引，表示从数组的结尾往前数 len(row) 个位置</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pad_features</span>(<span class="hljs-params">reviews_ints, seq_length</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27; eg: seq=3 [15,25,20,16,50] -&gt; [20, 16, 50]</span><br><span class="hljs-string">      [9] -&gt; [0,0,9]</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <br>    <span class="hljs-comment"># getting the correct rows x cols shape</span><br>    features = np.zeros((<span class="hljs-built_in">len</span>(reviews_ints), seq_length), dtype=<span class="hljs-built_in">int</span>)<br><br>    <span class="hljs-comment"># for each review, I grab that review and </span><br>    <span class="hljs-keyword">for</span> i, row <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(reviews_ints):<br>        features[i, -<span class="hljs-built_in">len</span>(row):] = np.array(row)[:seq_length]<br>    <br>    <span class="hljs-keyword">return</span> features<br></code></pre></td></tr></table></figure></li><li><p>标签数据的清洗</p><p>因为标签只有两类 negative 和  positive, 因此只要将其映射到 0 和 1 即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">训练标签的数据清洗</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>labels_split = labels.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>encoded_labels = [label <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels_split <span class="hljs-keyword">if</span> label]<br>encoded_labels = np.array([<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> label == <span class="hljs-string">&#x27;positive&#x27;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span> <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> encoded_labels])<br></code></pre></td></tr></table></figure></li></ul><h4 id="数据集的划分"><a href="#数据集的划分" class="headerlink" title="数据集的划分"></a>数据集的划分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">split_frac = <span class="hljs-number">0.8</span><br><br><span class="hljs-comment">## split data into training, validation, and test data (features and labels, x and y)</span><br>split_idx = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(features)*split_frac)<br>train_x, remaining_x = features[:split_idx], features[split_idx:]<br>train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]<br><br>test_idx = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(remaining_x)*<span class="hljs-number">0.5</span>)<br>val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]<br>val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]<br></code></pre></td></tr></table></figure><h4 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h4><p>​这次任务中模型的定义分成了三部分, 构造函数(搭建模型), 前递函数, 隐藏状态的初始化。</p><ul><li><p>构造函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Initialize the model by setting up the layers.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-variable language_">self</span>.output_size = output_size<br>        <span class="hljs-variable language_">self</span>.n_layers = n_layers<br>        <span class="hljs-variable language_">self</span>.hidden_dim = hidden_dim<br>        <br>        <span class="hljs-comment"># embedding and LSTM layers</span><br>        <span class="hljs-variable language_">self</span>.embedding = nn.Embedding(vocab_size, embedding_dim)<br>        <span class="hljs-variable language_">self</span>.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, <br>                            dropout=drop_prob, batch_first=<span class="hljs-literal">True</span>)<br>        <br>        <span class="hljs-comment"># dropout layer</span><br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(<span class="hljs-number">0.3</span>)<br>        <br>        <span class="hljs-comment"># linear and sigmoid layers</span><br>        <span class="hljs-variable language_">self</span>.fc = nn.Linear(hidden_dim, output_size)<br>        <span class="hljs-variable language_">self</span>.sig = nn.Sigmoid()<br></code></pre></td></tr></table></figure></li><li><p>前递函数定义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, hidden</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass of our model on some input and hidden state.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        batch_size = x.size(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># embeddings and lstm_out</span><br>        x = x.long()<br>        embeds = <span class="hljs-variable language_">self</span>.embedding(x)<br>        lstm_out, hidden = <span class="hljs-variable language_">self</span>.lstm(embeds, hidden)<br>        <br>        lstm_out = lstm_out[:, -<span class="hljs-number">1</span>, :] <span class="hljs-comment"># getting the last time step output</span><br>        <br>        <span class="hljs-comment"># dropout and fully-connected layer</span><br>        out = <span class="hljs-variable language_">self</span>.dropout(lstm_out)<br>        out = <span class="hljs-variable language_">self</span>.fc(out)<br>        <span class="hljs-comment"># sigmoid function</span><br>        sig_out = <span class="hljs-variable language_">self</span>.sig(out)<br>        <br>        <span class="hljs-comment"># return last sigmoid output and hidden state</span><br>        <span class="hljs-keyword">return</span> sig_out, hidden<br></code></pre></td></tr></table></figure></li><li><p>隐藏状态的初始化</p><p>这里是同时初始化了 c 和 h, 只不过全放在了一个 h 里面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_hidden</span>(<span class="hljs-params">self, batch_size</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27; Initializes hidden state &#x27;&#x27;&#x27;</span><br>        <span class="hljs-comment"># Create two new tensors with sizes n_layers x batch_size x hidden_dim,</span><br>        <span class="hljs-comment"># initialized to zero, for hidden state and cell state of LSTM</span><br>        weight = <span class="hljs-built_in">next</span>(<span class="hljs-variable language_">self</span>.parameters()).data<br>        <br>        <span class="hljs-keyword">if</span> (train_on_gpu):<br>            hidden = (weight.new(<span class="hljs-variable language_">self</span>.n_layers, batch_size, <span class="hljs-variable language_">self</span>.hidden_dim).zero_().cuda(),<br>                  weight.new(<span class="hljs-variable language_">self</span>.n_layers, batch_size, <span class="hljs-variable language_">self</span>.hidden_dim).zero_().cuda())<br>        <span class="hljs-keyword">else</span>:<br>            hidden = (weight.new(<span class="hljs-variable language_">self</span>.n_layers, batch_size, <span class="hljs-variable language_">self</span>.hidden_dim).zero_(),<br>                      weight.new(<span class="hljs-variable language_">self</span>.n_layers, batch_size, <span class="hljs-variable language_">self</span>.hidden_dim).zero_())<br>        <br>        <span class="hljs-keyword">return</span> hidden<br></code></pre></td></tr></table></figure></li></ul><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>​关于模型训练我想单独写一篇博客来讲一下, 不然在这详细写篇幅有点大。</p><div class="note note-info">            <p>​2024.7.16: 挖个坑, 等自己填, 我感觉应该不会太久, 关于 pytorch 的模型训练流程我想写好久了, 一直没机会写。</p>          </div><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><p>​模型评估有一个比较完整的体系, 不过我们这种小项目对于模型的评估其实就是自己编写一些示例来测试一下模型的性能, 看看泛化性怎么样。</p><ul><li><p>定义编码输入的句子函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_review</span>(<span class="hljs-params">test_review</span>):<br>    test_review = test_review.lower() <span class="hljs-comment"># lowercase</span><br>    <span class="hljs-comment"># get rid of punctuation</span><br>    test_text = <span class="hljs-string">&#x27;&#x27;</span>.join([c <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> test_review <span class="hljs-keyword">if</span> c <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> punctuation])<br><br>    <span class="hljs-comment"># splitting by spaces</span><br>    test_words = test_text.split()<br><br>    <span class="hljs-comment"># tokens</span><br>    test_ints = []<br>    test_ints.append([vocab_to_int.get(word, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> test_words])<br><br>    <span class="hljs-keyword">return</span> test_ints<br></code></pre></td></tr></table></figure></li><li><p>定义预测函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">net, test_review, sequence_length=<span class="hljs-number">200</span></span>):<br>    <br>    net.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-comment"># tokenize review</span><br>    test_ints = tokenize_review(test_review)<br>    <br>    <span class="hljs-comment"># pad tokenized sequence</span><br>    seq_length=sequence_length<br>    features = pad_features(test_ints, seq_length)<br>    <br>    <span class="hljs-comment"># convert to tensor to pass into your model</span><br>    feature_tensor = torch.from_numpy(features)<br>    <br>    batch_size = feature_tensor.size(<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-comment"># initialize hidden state</span><br>    h = net.init_hidden(batch_size)<br>    <br>    <span class="hljs-keyword">if</span>(train_on_gpu):<br>        feature_tensor = feature_tensor.cuda()<br>    <br>    <span class="hljs-comment"># get the output from the model</span><br>    output, h = net(feature_tensor, h)<br>    <br>    <span class="hljs-comment"># convert output probabilities to predicted class (0 or 1)</span><br>    pred = torch.<span class="hljs-built_in">round</span>(output.squeeze()) <br>    <span class="hljs-comment"># printing output value, before rounding</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Prediction value, pre-rounding: &#123;:.6f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(output.item()))<br>    <br>    <span class="hljs-comment"># print custom response</span><br>    <span class="hljs-keyword">if</span>(pred.item()==<span class="hljs-number">1</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Positive review detected!&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Negative review detected.&quot;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​这个项目是跟着github上一个教程写的, 在完成这个项目的过程中我接触到了很多以前没有考虑过的东西, 比如数据的清洗、剪枝和填充、word2int、word2vec等等。尤其是数据清洗的流程和小细节, 让我受益良多。虽然这个项目挺简单, 但是它确实增强了我对 NLP 任务 pipline 的理解和掌握。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>LSTM</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]LSTM学习</title>
    <link href="/2024/07/13/DL-LSTM%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/13/DL-LSTM%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="RNN的缺陷"><a href="#RNN的缺陷" class="headerlink" title="RNN的缺陷"></a>RNN的缺陷</h3><p>​    上一篇文章学习了 RNN 的原理与应用, 但是当人们应用起 RNN 时, 仍然有着很多缺陷, 比如在处理长序列时，尤其是需要捕捉远距离依赖关系的情况下, RNN表现不佳, 并且容易出现梯度消失与爆炸, 梯度消失与爆炸出现的原因推导可以参考<a href="https://blog.csdn.net/mary19831/article/details/129570030">这篇文章</a>。</p><p>​     远距离的依赖关系举个例子：“红烧排骨”出现在文字的开头，当输入到最后字符串是，RNN网络，可能已经忘记了本序列最重要的单词“红烧排骨”，…..的做法与辣子鸡相似，最后就有可能预测出“辣子鸡”。对于这个远距离依赖关系, 吴恩达的课上讲的通俗易懂, 可以看看。</p><h3 id="GRU-vs-LSTM"><a href="#GRU-vs-LSTM" class="headerlink" title="GRU vs LSTM"></a>GRU vs LSTM</h3><p>​     为了解决以上的问题, GRU 和 LSTM就出现了, GRU (Gate Recurrent Unit)译作门控循环单元, LSTM (Long Short-Term Memory) 译作长短时记忆。这里就不详细介绍 GRU 的原理了, 如果将来我回看时发现需要补充 GRU时, 可以参考一下<a href="https://zhuanlan.zhihu.com/p/32481747">这篇文章</a>。</p><p>​     这里比较一下 GRU 和 LSTM。</p><table><thead><tr><th align="center">网络结构</th><th align="center">LSTM</th><th align="center">GRU</th></tr></thead><tbody><tr><td align="center">复杂度</td><td align="center">结构复杂</td><td align="center">结构简单</td></tr><tr><td align="center">参数量</td><td align="center">参数量大</td><td align="center">参数量小</td></tr><tr><td align="center">计算开销</td><td align="center">大</td><td align="center">小</td></tr><tr><td align="center">灵活度</td><td align="center">更灵活</td><td align="center">较差</td></tr></tbody></table><p>在RNN实现的任务中，不考虑计算资源限制的情况下，常将 LSTM 作为默认选项。</p><h3 id="LSTM-原理"><a href="#LSTM-原理" class="headerlink" title="LSTM 原理"></a>LSTM 原理</h3><p>​     需要注意的是, LSTM 并不是一种全新的、颠覆性的架构，它仍然是在 RNN 的基础上发展来的，LSTM的每一个cell 是 RNN 的每一个 cell 的增强版。</p><p>​     LSTM在原来RNN的基础上增加了更新门、遗忘门、输出门。</p><p><img src="/images/post_3/11.png" alt="LSTM原理图"></p><p>​     从图中可以看到, 相比 RNN, LSTM 的输入由2个(a和x)变为了3个(a&#x2F;x&#x2F;c)，图中的 c~^(t)就是原来 RNN的输出a^(t)。但是在 LSTM 中, a与x要经过更新门、遗忘门、输出门三个门再与c^(t-1)作相应的运算才能得到a^t、c^t, a^t经过 softmax 得到输出 y^t。计算过程如下图所示(是从吴恩达老师课上截的图，但是真的易懂)</p><p><img src="/images/post_3/10.png" alt="LSTM计算过程"></p><p>​    从图中可以得到需要学习的参数, 原来 RNN 的 Wa、Ba变成了 Wc、Bc。又新增了 Wu、Bu、Wf、Bf、Wo、Bo这六个权重。</p><div class="note note-info">            <p>​2024.7.13: 渐渐地懂了序列模型的一般结构, 感觉 LSTM 的思想好牛啊, 感觉已经是当时局限思想情况下的一个极致的时间序列模型了</p>          </div><p>​      再结合 BRNN 的结构出现了双向的 LSTM 网络结构, 目前使用 RNN 模型大部分都是双向的 LSTM 结构，有一种集百家之所长的感觉。双向 LSTM 结构就不细讲了，知道了 BRNN 和 LSTM 结构很容易就能理解 LSTM 结构。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​    从 RNN 到 LSTM, 原来模型的发展从来都不是一蹴而就的, 人们永远无法全面的预测到当前提出的模型所存在的所有缺陷, 模型的发展给我一种问题导向的感觉, CNN 的出现解决了图像分类的问题, 但是时序预测还卡着人们, 接着出现了 RNN, 但是 RNN 的出现暴露了很多缺陷, 继而又出现了 LSTM, LSTM 虽然解决了 RNN的一些问题，但是仍未跳脱出时序顺序的输入, 可能当时的人们也认为时序顺序的输入是解决一些时序预测问题的最佳解决策略了, 后来 transform 的出现又颠覆了这一认知。</p><blockquote><p>​    “那未来又会发生什么呢？”</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]RNN学习</title>
    <link href="/2024/07/13/DL-RNN%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/13/DL-RNN%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="RNN介绍"><a href="#RNN介绍" class="headerlink" title="RNN介绍"></a>RNN介绍</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>​    RNN(Recurrent Neural Network)一般中文译作循环神经网络，是一种序列模型，常用来处理 nlp 等领域的多种任务，包括但不限于语言生成、机器翻译、语音识别、音乐生成等等。在 nlp 领域的发展起了很重要的作用，也是很多研究者必要掌握的模型。</p><p>​    <img src="/images/post_3/2.png" alt="RNN应用领域"></p><p>​    RNN 与 视觉领域的模型(如CNN)的很大区别在于， RNN 网络是将时序因素考虑在内的，在学习RNN时常见”时间步(time step)”这一概念。举例来说，在训练图像分类模型时，图片输入模型的顺序并无要求，但是在训练序列模型时，训练数据输入的顺序有了必要要求。</p><h3 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h3><p>​    下面介绍的模型训练数据默认为 one-hot 编码。</p><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>​    通常情况下 RNN 架构的网络不会有很多层(一般不大于3层), 下面以一层举例, 每一层的输入是 a^t 和 x^t 这两个变量,  输出是 a^(t+1) 和 y^t ,  y^t 是由 sigmoid 激活函数激活的, 表示此处为各个 word 的概率, a^(t+1) 一般是由 tanh 激活函数激活的, 作为下一时间步网络的输入与 x^(t+1) 一同输入模型, 依此一直循环。a^(t+1) 和 y^t 在图中给出。</p><p>​    <img src="/images/post_3/1.png" alt="RNN前向传播原理"></p><p>​     注意上面给出的原理图只是为了方便理解每个时间步干了什么, 把循环展开了, 真正的 RNN 结构图其实是循环的。</p><p>​    <img src="/images/post_3/14.png" alt="RNN原理图"></p><p>​     从前向传播原理图中可以得出在训练过程中模型学习的参数是 Wa、Wy、Ba、By。</p><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>​    了解模型的反向传播可以帮我们更好的理解模型学习的过程, 其实单纯是我把反向传播过程的一些细节给忘了, 想回忆一下。关于反向传播基本概念和原理可以参考一下<a href="https://zhuanlan.zhihu.com/p/261710847">这篇文章</a>。下图是关于 RNN 的反向传播过程。</p><p>​    <img src="/images/post_3/3.png" alt="RNN反向传播过程"></p><p>​     图中的红色箭头表明的反向传播路径。简单来说就是定义一个损失函数(有的文章叫做误差函数)，根据前向传播得到的输出与实际值计算损失函数，再通过链式法则对权重(在这里是Wa\Wy\Ba\By)进行求导。</p><p>​     将原来的权重值 — 求导的结果*lr 得到新的权重完成梯度下降的过程。</p><h3 id="不同输出的-RNN-网络"><a href="#不同输出的-RNN-网络" class="headerlink" title="不同输出的 RNN 网络"></a>不同输出的 RNN 网络</h3><p>​    在RNN应用领域那张图上可以看到 RNN 有很多应用领域, 模型的输入和输出并不一定是严格一对一的，特定的任务情境下 RNN 网络的输出数量并不一定等于输入的数量, 因此要根据特定任务场景调整网络的输出。</p><h4 id="一对多网络"><a href="#一对多网络" class="headerlink" title="一对多网络"></a>一对多网络</h4><p>   一对多常用在生成任务上, 比如音乐的生成, 输入可以为空或者一个表示音乐基调的单词, 输出则为不等长的音符组成音乐，这种网络结构如下图所示。</p><p>​        <img src="/images/post_3/6.png" alt="One2Many"></p><h4 id="多对一网络"><a href="#多对一网络" class="headerlink" title="多对一网络"></a>多对一网络</h4><p>​    多对一网络常用在舆论检测上, 比如饭店评价通过输入一段话来评判客户对这家饭店的打星, 比如打星范围为1-5, 那么模型的唯一输出则为打星值。这种网络结构如下图所示。</p><p>​    <img src="/images/post_3/7.png" alt="Many2One">    </p><h4 id="多对多网络"><a href="#多对多网络" class="headerlink" title="多对多网络"></a>多对多网络</h4><p>​    这种网络最常见, 有时候输入的序列长度跟输出序列长度并不相等, 比如机器翻译应用。这种网络结构如下图所示，我们常将带有 x 输入的模型的前半部分叫做 encoder(编码器), 带有 y 输出的模型后半部分叫做 decoder(译码器)。</p><p>​    <img src="/images/post_3/15.png" alt="Many2Many">        </p><h3 id="双向-RNN-BRNN"><a href="#双向-RNN-BRNN" class="headerlink" title="双向 RNN (BRNN)"></a>双向 RNN (BRNN)</h3><p>   RNN 网络虽然解决了序列输入的训练问题，但是如果碰到类似于下面这种训练输入情况</p><figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk"><span class="hljs-comment">&quot;Hello, Teddy Bear is on sale.&quot;</span><br><span class="hljs-comment">&quot;Hello, Teddy is coming.&quot;</span><br></code></pre></td></tr></table></figure><p>   RNN 在序列预测时, 如果前面的三个词是 “Hello, Teddy”, 这时候模型很难正确预测下面的输出是什么，这也是单向 RNN 网络的局限性，为了解决这种问题，出现了双向循环网络(Bidirectional RNN), 从左向右与从右向左同时开始训练。原理图如下所示。</p><p>​    <img src="/images/post_3/12.png" alt="BRNN原理图">            </p><h3 id="深层-RNN-DRNN"><a href="#深层-RNN-DRNN" class="headerlink" title="深层 RNN (DRNN)"></a>深层 RNN (DRNN)</h3><p>   在构建 RNN 网络时, 因为时序循环的存在使得网络本身就具有了很多层的网络, 因此 RNN 的层数一般没有很多，但当训练数据量增大时，深层 RNN (Deep RNN) 的效果比单层的 RNN 效果往往更好。深层 RNN 的激活向量的运作如下图所示, 就是单纯地给每层都像以前一样重复操作。</p><p>​    <img src="/images/post_3/13.png" alt="DRNN原理图">     </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>   RNN的出现极大推动了NLP领域的发展, 最后用科学家们对其评价来结尾。</p><blockquote><p>“RNN和其变种（如LSTM、GRU）在处理时间序列和序列数据方面具有巨大的潜力，尤其是在语音识别、翻译、语音生成等领域。然而，Transformer模型的出现正在改变这一领域的游戏规则，Transformer在处理长序列数据时展示出了更高的效率和性能。”</p><p>​                                                                                                                                                            ——Andrew Ng</p></blockquote><blockquote><p>“尽管RNN在很多应用中表现出色，但在处理长序列时，尤其是需要捕捉远距离依赖关系的情况下，Transformer等新型架构可能会更加适合。”</p><p>​                                                                                                                                                        ——Ilya Sutskever</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]Self-attention学习</title>
    <link href="/2024/07/10/DL-Self-attention%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/10/DL-Self-attention%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="新的任务场景"><a href="#新的任务场景" class="headerlink" title="新的任务场景"></a>新的任务场景</h4><p>​    在前面的分类模型(如CNN等)的学习中, 模型的输入(如图像)是一个固定大小向量且在批输入时的各个输入向量通常并无直接关系, 有些任务下(如文字处理)需要变长的向量作为模型的输入, 且输入的各个向量之间有一定的语义关系，这时就要考虑改进模型以适应这种任务的需求。</p><p>​    通常情况下这种模型的输出有三种情况(如下图所示):    </p><p><img src="/images/post_2/5.png" alt="模型输出的三种情况"></p><ul><li><p>输入的每个向量都对应一个输出 seqence labeling</p><p>常用于词性标注等任务</p></li><li><p>输入的n个变量只对应一个输出</p><p>常用于舆情分析(从一句话中抽取对某事的态度)、药物毒性检测。</p></li><li><p>输入的n个变量对应m个输出 seq2seq</p><p>应用最广泛，机器翻译，分子预测，对话任务等。</p></li></ul><h4 id="新的问题"><a href="#新的问题" class="headerlink" title="新的问题"></a>新的问题</h4><p>​    对 Sequence Labeling 而言，我们想要n个输入的变长向量对应n个输出，我们自然而然的想到可以给每个向量都加上一个 FC 层，这样可以得到n个输出向量，但是这样就完全把语义信息抛弃了，词与词之间的关系没有用上。像 “I saw a saw”这个例子，saw既有动词”看”的意思, 又有名词”锯子”的意思，但是如果你输入同样的 FC 层，输出肯定只能有一种结果。</p><h3 id="Self-Attention-原理"><a href="#Self-Attention-原理" class="headerlink" title="Self-Attention 原理"></a>Self-Attention 原理</h3><h4 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h4><p>​    简单来说， Self-Attention 就是把各个词的相关度也作为输入输进了模型，如下图</p><p><img src="/images/post_2/4.png" alt="将相关度作为输入"></p><p>​     然后再将输出作为输入进入 FC 层(可以再将FC层的输出作为输入再进self-attention层)</p><p><img src="/images/post_2/1.png" alt="Self-Attention的总览"></p><h4 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h4><p>​    目前主流算相关度的方法是Google提出的 Dot-product 方法，以算 a1 与 a2、 a3、 a4 的相关度为例:</p><ul><li><p>先算a1的q(quary, 查找的意思, 去找 a1 与其他输入的相关度) q &#x3D; Wq*a1 (??? 这里的Wq咋来的，是一个标量还是一个向量，算每个输入的 q 时这个Wq是一样的吗)</p></li><li><p>再算其他输入的k, k &#x3D; Wk * an (??? 这里的Wk也同问)，这里也有必要算一下 a1 的 k</p></li><li><p>将a1的q与每个k作点积运算，得到相关系数a1,1 &#x2F; a1,2 等等</p></li><li><p>将得到的相关系数经过一层softmax(不一定非要softmax, ReLU等也可以)得到a’1,1 &#x2F; a’1,2 等等</p><p> <img src="/images/post_2/2.png" alt="求相关度原理图"></p></li><li><p>算每个输入的 v &#x3D; Wv * a1(??? 这里的Wv也同问)</p></li><li><p>将每个 v与对应的 a’ 相乘再相加得到 b1, 如果 b1 很接近 v 与 a’1,2 的结果，那么就跟输入2更相关</p><p> <img src="/images/post_2/3.png" alt="求b的原理图"></p></li></ul><h4 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><ul><li><p>将各个input vector 合成矩阵 I, 李宏毅老师的 ppt 很明了, 竖着放置相当于将各个 vector 竖列组合。再将其分别与Wq、Wk、Wv作矩阵相乘运算, 得到 Q、 K、 V。每列即为算得的qi、ki、vi。</p><p><img src="/images/post_2/6.png" alt="求矩阵QKV"></p></li><li><p>再将 K.T 与 Q 作矩阵乘法, 得到A, 经过softmax得到A’, A’ 的第n行第m列即为 自注意力score a’n,m。</p><p><img src="/images/post_2/7.png" alt="求A&#39;"></p></li><li><p>再将 V 与矩阵 A’ 作矩阵乘法, 得到矩阵 O, 每一列即为 bi。</p><p><img src="/images/post_2/8.png" alt="求A&#39;"></p></li><li><p>上面整个流程如下图所示, 这个流程中所需要学习的参数就是 Wq、Wk、Wv。</p><p><img src="/images/post_2/9.png" alt="self-attention矩阵运算流程"></p></li></ul><h3 id="Self-Attention-vs-CNN"><a href="#Self-Attention-vs-CNN" class="headerlink" title="Self-Attention vs CNN"></a>Self-Attention vs CNN</h3><p>   研究表明, Self-Attention 是一中更加 flexible 的 CNN, 而CNN 则是受限的 Self-Attention。self-attention是可以通过调整某些参数变成CNN的。</p><p><img src="/images/post_2/13.png" alt="Self-Attention vs CNN"></p><h3 id="Muti-head-Self-Attention"><a href="#Muti-head-Self-Attention" class="headerlink" title="Muti-head Self-Attention"></a>Muti-head Self-Attention</h3><p>   关于多头注意力机制，其思路就是将每个q、k、v分成多路，然后照着一样的流程各自去算矩阵O’，最后再将矩阵O’通过运算化成矩阵O。</p><p>   为了方便理解，可以这样想为什么要多头注意力机制: 如果只用一头注意力机制，那么这个q就负责这所有的相关性信息的搜索。如果用多头注意力机制，那么每个q就可以专门负责某一方面的搜寻，使得训练得更全面。</p><p><img src="/images/post_2/10.png" alt="多头注意力机制的原理图"></p><p>​    将q分成多路的运算: [q1,1 , q1,2] &#x3D; W_q_muti x q</p><p>​    K、V分成多路的运算也同理。</p><p>​    这样需要学习的参数又多了 W_q_muti、W_k_muti、W_v_muti、W_o_muti(形状不同前面三个, 作用是将多个O’合成一个O,如图)。</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>​    上面提到的 Self-Attention 虽然将各个input vector 之间的关系考虑了进来, 但是仍然忽略了各个input vector 的位置关系。比如训练数据为语言的时候，最前面位置的词一般是主语。这时候就要考虑位置的因素了，这时候可以将每个输入向量加上对应的位置向量ei, 再进行self-attention。</p><p><img src="/images/post_2/11.png" alt="Positional Encoding"></p><p>​    目前有种 ei 的实现方法, transformer 那篇论文中提出的 Postional Encoding 是一种手工实现的 ei, 事先将每个位置的 ei 都规定好了。当然还有将 ei 设置为待学习的参数进行学习的方法。</p><p>​     <a href="https://blog.csdn.net/m0_37605642/article/details/132866365#:~:text=%E6%9C%AC%E8%8A%82%E4%BB%A5%20Sinusoidal">transform中的Postional Encoding</a></p><h3 id="Self-Attention的应用"><a href="#Self-Attention的应用" class="headerlink" title="Self-Attention的应用"></a>Self-Attention的应用</h3><h4 id="Self-Attention-for-Speech"><a href="#Self-Attention-for-Speech" class="headerlink" title="Self-Attention for Speech"></a>Self-Attention for Speech</h4><p>​    语音一般是一个很长的序列, 如果将这个序列中的每个vector的关系都算score的话, 计算量会很大, 所以一般会将计算的范围限制在一定长度。</p><p><img src="/images/post_2/12.png" alt="Self-Attention for Speech"></p><h4 id="Self-Attention-for-Image"><a href="#Self-Attention-for-Image" class="headerlink" title="Self-Attention for Image"></a>Self-Attention for Image</h4><p>   图像也可以作为序列做self-attention。一般输入的图像都是RGB通道的，这时每个 pixel 包含的三个值可以作为sequence的一个vector。这时候sequence的长度取决于图像的大小。</p><p><img src="/images/post_2/13.png" alt="Self-Attention for Image"></p><h4 id="Self-Attention-for-Graph"><a href="#Self-Attention-for-Graph" class="headerlink" title="Self-Attention for Graph"></a>Self-Attention for Graph</h4><p>   图也可以做self-attention, 这时候只需要考虑每个结点与之相连结点的score即可。</p><p><img src="/images/post_2/15.png" alt="Self-Attention for Graph"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>注意力机制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>畸形的内卷</title>
    <link href="/2024/07/09/%E7%95%B8%E5%BD%A2%E7%9A%84%E5%86%85%E5%8D%B7/"/>
    <url>/2024/07/09/%E7%95%B8%E5%BD%A2%E7%9A%84%E5%86%85%E5%8D%B7/</url>
    
    <content type="html"><![CDATA[<h3 id="内卷与方向"><a href="#内卷与方向" class="headerlink" title="内卷与方向"></a>内卷与方向</h3><h4 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h4><p>​今天下午在调飞机闲聊时，刚好聊到了分流的事情，学妹让我给些建议(当然其实我觉得我只能给出每个专业的利弊，最后的定夺是应该结合自己的人生目标选择的)。聊着聊着学弟就聊起了他的舍友：</p><blockquote><p>“我舍友上学期几门课卡了3.9的绩点，最后磨老师全给改成了4.1的绩点。”</p></blockquote><p>​我其实挺看不起这种行为的，大家本来都是一起学的，凭什么你能把绩点给改了，这对别人一点也不公平。</p><p>​其实上面的见怪不怪，但是最让我震惊的是学弟学妹竟然也想这样做，讨论让老师给自己改成绩的方法。我心想我天，这都是什么鬼(先叠一下，假如这篇博客不小心被学弟学妹看到了，不要怪我，在这给你们道个歉)。</p><h4 id="看法"><a href="#看法" class="headerlink" title="看法"></a>看法</h4><p>​聊聊我对这些事的看法吧，先声明是拙见。</p><p>​目前高考之前的教育体制让学生们都变得太”唯分数论”了，他们自信地以为只要我的绩点足够高，那么万事万物都是美好的。可是，然后呢？绩点刷到 4.0x 然后呢？</p><p>​你说，然后我就保研了啊。可是，保研了之后呢？再在研究生阶段接着卷绩点吗？</p><blockquote><p>“你始终都在被绩点、综测推着去做事情，你始终不知道你想要的是什么”</p></blockquote><p>​我发现这种现象越来越严重了，大学慢慢的变成了另一所高中。诚实的说，我在大一的时候也是这样的想法:为什么加基地呢？因为可以加综测(歪一下，但是当时面试的时候我还是说的是因为有很多厉害的学长学姐，可以向他们学习，现在看来太难绷了 hhhhh )。</p><p>​虽然之前我也这样想，但是在基地的这一年多，我慢慢的清楚了自己想要的是什么，我会衡量一下为了实现想要的，我要不要去努力的卷绩点、综测，我应该做些什么去实现我想要的，我应该把绩点、综测搞到什么程度就可以了，毕竟这东西在大学非常耗费时间和经历。</p><p>​这样卷搞得大学生活真的没滋没味，太难受了，至少我的前两年是这样觉得的。</p><h4 id="难以破局"><a href="#难以破局" class="headerlink" title="难以破局"></a>难以破局</h4><p>​其实我觉得只要弄清你要什么，你想得到什么，很容易破局。如果你最终是要去公司工作，这时候你就要考虑一下读研对你的工作到底有没有实质性的帮助。其实我感觉三年的工作经验跟读研三年出来工作很难衡量哪个更好。为什么说难以破局，很多人可能跟我一样，想试试自己有没有科研的潜力，再决定自己未来的道路，这时候就要不得不卷一下或者考研了。哎，很难说。</p><p>​但其实我觉得一个人最终能达到什么样的高度，跟自己的性格高度相关，你如果不知道该咋办了，那就听从自己的内心吧(就是做自己觉得应该做的)，每个人都不一样也很难说。</p><h4 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h4><p>​原来这里有很多话想说，但是我都删了。</p><p>​想家了。</p><table><thead><tr><th>觉得赵雷的《<a href="https://music.163.com/#/song?id=447926063">朵</a>》很好听 特别是前奏！</th></tr></thead></table>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>内卷</tag>
      
      <tag>人生目标</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="/2024/07/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2024/07/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h3 id="我的第一篇博客"><a href="#我的第一篇博客" class="headerlink" title="我的第一篇博客"></a>我的第一篇博客</h3><h5 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h5><p>​纪念一下自己的第一篇博客。感觉我自己还算是一个喜欢分享的人(maybe)，虽然我一直发私密的动态。暑期在做集训的间隙搭建了这个博客网站，感觉还不错，就当QQ空间&#x2F;朋友圈用了。</p><p>​网站是跟着csdn上的教程搭的，部署在github上，基本上不费工夫，搭搭积木一样。搭好之后对网站进行改造有一种小时候玩4399换装的感觉。</p><h5 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h5><p>​我为什么搭建这个博客？我们从初中就开始学习历史，历史事件的发生一般有三种原因: 根本原因、直接原因、导火索。</p><p>​我觉得我搭这个网站的根本原因就是自己是一个喜欢记录的人，但是又不想让别人看见，所以搭这个博客正好满足了我的需求，不出意外的话，这网站我是不会给朋友们说的。直接原因就是今年无人机视觉太难做了，反复折磨，想着记录一下被折磨的过程。导火索是看到wh学长的博客，很早之前写的博客还有callback，很戳我，我很喜欢这样的记录方式。所以立马就搭了这个博客。</p><h5 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h5><p>​不出意外的话，以后我对生活的一些感悟和经历、一些想法、学习的过程、对技术的理解(虽然也没啥理解)，都会在这里更新，希望我自己能坚持吧。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>流水账的生活,偶尔也闪烁</title>
    <link href="/2024/07/07/%E6%B5%81%E6%B0%B4%E8%B4%A6%E7%9A%84%E7%94%9F%E6%B4%BB-%E5%81%B6%E5%B0%94%E4%B9%9F%E9%97%AA%E7%83%81/"/>
    <url>/2024/07/07/%E6%B5%81%E6%B0%B4%E8%B4%A6%E7%9A%84%E7%94%9F%E6%B4%BB-%E5%81%B6%E5%B0%94%E4%B9%9F%E9%97%AA%E7%83%81/</url>
    
    <content type="html"><![CDATA[<h1 id="流水账的生活-偶尔也闪烁"><a href="#流水账的生活-偶尔也闪烁" class="headerlink" title="流水账的生活,偶尔也闪烁"></a>流水账的生活,偶尔也闪烁</h1><p>​灵感来自于别人博客记录生活, 时不时还会有 callback 的这种记录方式我非常喜欢, 因此创建此篇博客来记录我的生活, 虽然有些流水账, 但是偶尔也是有那么几点闪光的。</p><h2 id="2024"><a href="#2024" class="headerlink" title="2024"></a>2024</h2><h3 id="7-月"><a href="#7-月" class="headerlink" title="7 月"></a>7 月</h3><p>​<em><strong><u>7.17</u></strong></em>: 中午的时候无意间看了一部剧《少年歌行》, 从第一集就开始高能, 诶其实我觉得有些电视剧能抓人的点就在于他在前几集就能抓住人心让人想看下去, 有些剧情矛盾在5、6集才开始显现的平铺直叙的剧反而对我没有太大的吸引的点。(这部剧好看到我甚至翘了一个班去追, 一下午看了10集, 以后得控制一下, 不然吃饭的时候没看的了……)</p><div class="note note-info">            <ul><li>2024.7.21: 在昨天把这部剧追完了, 感觉心里空落落的, 不得不吐槽一下这个男主的演技真的烂, 全程一个表情, 我不好评价, 反正我不喜欢, 但是这个剧情真的很赞！环环相扣, 大概是我看过的古装仙侠剧里面one of the best了。萧瑟、无心、雷无桀、大师兄唐莲、司空千落…….. 江湖再见！</li></ul>          </div><p>​<em><strong><u>7.21</u></strong></em>: 准备了好久的比赛终于完成了(5月初-7月21), 有惊无险, 满分拿下。过程真的很惊险, 比赛是20号晚上8点进行的, 飞机飞到二维码处突然不动了, 只好接管飞机, 第一次起飞草草了事了。后来把飞机的高度调高才刚刚好识别出来, 第二次满分拿下了, 实在是万万没想到会在这里出问题。 事后复盘我感觉应该是二维码的三个定位点在晚上被灯照得无法确定了。总的来说还是有惊无险吧, 期待江苏的国赛会怎样呢？</p><p>​看到了优酷的少白这部剧, 是少歌的前传, 看了前面几集感觉还行, 准备明天回家的高铁上看一下。希望明天的高铁上有插座, 不然我手机没电了可完蛋了。</p><p>​<em><strong><u>7.23</u></strong></em>: 昨晚上十点回到家后收拾收拾就睡了,  现在正好有时间, 记录一下这两天的生活。今天早上, 去县医院困扰我已久的牙疼治好了, 直接把那颗坏了的智齿给拔掉了, 医生人挺好, 动作也很利索, 给我打了三针麻药(没想象中那么疼)就用钳子给我卸下来了, 整个过程也就不到两分钟, 现在麻药劲过去了, 自我感觉良好。中午吃了火锅, 在学校没有吃过, 没人陪我吃。下午去大姨家看望姥爷, 聊了一些家长里短, 突然有点感慨时间过得好快啊, 两年之后我再看到我自己的这篇博客会在想什么呢？爷爷收养了一直小猫, 通体灰色的, 我一直对猫狗不是很喜欢, 但也谈不上讨厌, 以往家里的猫狗失踪或是去世我其实没太大感觉, 但是这个小灰猫给了我不一样的感觉。后面的几天开启田园生活了。</p><p>​<em><strong><u>7.27</u></strong></em>: 终于回归了！应该是前天, 给我的电脑换了一个2T的硬盘, 终于不用再为没有空间发愁了, 装完 ubuntu 后给根目录分了200G, 给home目录分了500G,我也不知道自己会不会后悔, 就是说这个home目录有必要分这么大吗, 全给到根目录不行吗? (是跟着网上的教程分的)不是很理解, 将来也不知道我会不会后悔, 我会把换硬盘的经历和教训写一篇博客以供后面参考, 这几天看了《唐朝诡事录》, 感觉还挺好看的。我要把搜狗输入安装一下, 太难受了这个自带的输入法！</p><h3 id="8-月"><a href="#8-月" class="headerlink" title="8 月"></a>8 月</h3><p>​<em><strong><u>8.1</u></strong></em>: 这几天把新装的 ubuntu 系统拾掇了一遍, 把该装的都装了一下, 现在的 ubuntu 感觉比上一个好用多了。 昨天姑姑姐姐来家里玩了, 叙了叙旧。这几天有在关注巴黎奥运会, 为奥运健儿们加油！ 《唐诡》已经看到第二部了, 说实话真的不错, 最近一直没看 《少白》, 打算留着在高铁上打发时间再看, 但最近看这部剧的风评急转直下, 挺难评的。抽象组委会竟然改比赛时间地点了, 一下提前了几天, 真无语, 我在想我比完赛要不要回家, 我应该会回家的吧。要去衡水了, 祝我顺利。不说了, 去训模型了。</p><p>​<em><strong><u>8.5</u></strong></em>: 2号见了她, 很喜欢。昨天去了大姨家, 但是下了暴雨没能给姥姥上坟, 也没回来家, 索性就在大姨家住了一晚。昨晚上小丁哥给我们烤串，除了有点咸之外味道很好！ 今天拿了几件快递, 买了一些礼物。《唐诡西行》看完了, 我心里的 top1 直接投给供养人, top2 给到仵作之死, top3 是上仙坊的来信。供养人无论从立意、剧情、演技、节奏、合理性上面, 都堪称一绝, 很喜欢, 可怜的多宝。仵作之死独孤羊好惨, “独孤苍苍而娘子青青”, 休书那一段真的很感人.。上仙坊不用多说, 这个李云罪该万死。这几天发生的事情对我来说还挺特别的, 几个月以后一定要 callback 一下！</p><p>​<em><strong><u>8.10&lt;七夕&gt;</u></strong></em>: 今天成了有对象的人嘻嘻嘻嘻嘻嘻。在感情中主动的这门必修课, 我好像永远学不会, 但是你一直在给我台阶, 教我主动。第一封<a href="https://sigmoidsee.github.io/2024/08/11/%E6%88%91%E5%86%99%E7%BB%99%E4%BD%A0%E7%9A%84%E7%AC%AC%E4%B8%80%E5%B0%81%E6%83%85%E4%B9%A6/">情书</a>, 第一次牵手……未来我们的故事会怎样发展呢？</p><p>​<em><strong><u>8.14:</u></strong></em>  - 昨天晚上被鬼压床了, 这种感觉真是太难受了, 想动也动不了, 想喊也喊不出, 还体验到了灵魂出窍的感觉, 哎, 这个世界很难让我唯物主义…..少白今天大结局了, 一点都不好看, 把好多剧情都剪掉了！后天就去比赛了, 会发生什么呢……希望一切顺利吧……</p><p>​</p>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>琐事</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
