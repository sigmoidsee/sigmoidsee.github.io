<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>[DL]LSTM学习</title>
    <link href="/2024/07/13/DL-LSTM%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/13/DL-LSTM%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="RNN的缺陷"><a href="#RNN的缺陷" class="headerlink" title="RNN的缺陷"></a>RNN的缺陷</h3><p>​    上一篇文章学习了 RNN 的原理与应用, 但是当人们应用起 RNN 时, 仍然有着很多缺陷, 比如在处理长序列时，尤其是需要捕捉远距离依赖关系的情况下, RNN表现不佳, 并且容易出现梯度消失与爆炸, 梯度消失与爆炸出现的原因推导可以参考<a href="https://blog.csdn.net/mary19831/article/details/129570030">这篇文章</a>。</p><p>​     远距离的依赖关系举个例子：“红烧排骨”出现在文字的开头，当输入到最后字符串是，RNN网络，可能已经忘记了本序列最重要的单词“红烧排骨”，…..的做法与辣子鸡相似，最后就有可能预测出“辣子鸡”。对于这个远距离依赖关系, 吴恩达的课上讲的通俗易懂, 可以看看。</p><h3 id="GRU-vs-LSTM"><a href="#GRU-vs-LSTM" class="headerlink" title="GRU vs LSTM"></a>GRU vs LSTM</h3><p>​     为了解决以上的问题, GRU 和 LSTM就出现了, GRU (Gate Recurrent Unit)译作门控循环单元, LSTM (Long Short-Term Memory) 译作长短时记忆。这里就不详细介绍 GRU 的原理了, 如果将来我回看时发现需要补充 GRU时, 可以参考一下<a href="https://zhuanlan.zhihu.com/p/32481747">这篇文章</a>。</p><p>​     这里比较一下 GRU 和 LSTM。</p><table><thead><tr><th align="center">网络结构</th><th align="center">LSTM</th><th align="center">GRU</th></tr></thead><tbody><tr><td align="center">复杂度</td><td align="center">结构复杂</td><td align="center">结构简单</td></tr><tr><td align="center">参数量</td><td align="center">参数量大</td><td align="center">参数量小</td></tr><tr><td align="center">计算开销</td><td align="center">大</td><td align="center">小</td></tr><tr><td align="center">灵活度</td><td align="center">更灵活</td><td align="center">较差</td></tr></tbody></table><p>在RNN实现的任务中，不考虑计算资源限制的情况下，常将 LSTM 作为默认选项。</p><h3 id="LSTM-原理"><a href="#LSTM-原理" class="headerlink" title="LSTM 原理"></a>LSTM 原理</h3><p>​     需要注意的是, LSTM 并不是一种全新的、颠覆性的架构，它仍然是在 RNN 的基础上发展来的，LSTM的每一个cell 是 RNN 的每一个 cell 的增强版。</p><p>​     LSTM在原来RNN的基础上增加了更新门、遗忘门、输出门。</p><p><img src="/images/post_3/11.png" alt="LSTM原理图"></p><p>​     从图中可以看到, 相比 RNN, LSTM 的输入由2个(a和x)变为了3个(a&#x2F;x&#x2F;c)，图中的 c~^(t)就是原来 RNN的输出a^(t)。但是在 LSTM 中, a与x要经过更新门、遗忘门、输出门三个门再与c^(t-1)作相应的运算才能得到a^t、c^t, a^t经过 softmax 得到输出 y^t。计算过程如下图所示(是从吴恩达老师课上截的图，但是真的易懂)</p><p><img src="/images/post_3/10.png" alt="LSTM计算过程"></p><p>​    从图中可以得到需要学习的参数, 原来 RNN 的 Wa、Ba变成了 Wc、Bc。又新增了 Wu、Bu、Wf、Bf、Wo、Bo这六个权重。</p><div class="note note-info">            <p>​2024.7.13: 渐渐地懂了序列模型的一般结构, 感觉 LSTM 的思想好牛啊, 感觉已经是当时局限思想情况下的一个极致的时间序列模型了</p>          </div><p>​      再结合 BRNN 的结构出现了双向的 LSTM 网络结构, 目前使用 RNN 模型大部分都是双向的 LSTM 结构，有一种集百家之所长的感觉。双向 LSTM 结构就不细讲了，知道了 BRNN 和 LSTM 结构很容易就能理解 LSTM 结构。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>​    从 RNN 到 LSTM, 原来模型的发展从来都不是一蹴而就的, 人们永远无法全面的预测到当前提出的模型所存在的所有缺陷, 模型的发展给我一种问题导向的感觉, CNN 的出现解决了图像分类的问题, 但是时序预测还卡着人们, 接着出现了 RNN, 但是 RNN 的出现暴露了很多缺陷, 继而又出现了 LSTM, LSTM 虽然解决了 RNN的一些问题，但是仍未跳脱出时序顺序的输入, 可能当时的人们也认为时序顺序的输入是解决一些时序预测问题的最佳解决策略了, 后来 transform 的出现又颠覆了这一认知。</p><blockquote><p>​    “那未来又会发生什么呢？”</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]RNN学习</title>
    <link href="/2024/07/13/DL-RNN%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/13/DL-RNN%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="RNN介绍"><a href="#RNN介绍" class="headerlink" title="RNN介绍"></a>RNN介绍</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>​    RNN(Recurrent Neural Network)一般中文译作循环神经网络，是一种序列模型，常用来处理 nlp 等领域的多种任务，包括但不限于语言生成、机器翻译、语音识别、音乐生成等等。在 nlp 领域的发展起了很重要的作用，也是很多研究者必要掌握的模型。</p><p>​    <img src="/images/post_3/2.png" alt="RNN应用领域"></p><p>​    RNN 与 视觉领域的模型(如CNN)的很大区别在于， RNN 网络是将时序因素考虑在内的，在学习RNN时常见”时间步(time step)”这一概念。举例来说，在训练图像分类模型时，图片输入模型的顺序并无要求，但是在训练序列模型时，训练数据输入的顺序有了必要要求。</p><h3 id="原理介绍"><a href="#原理介绍" class="headerlink" title="原理介绍"></a>原理介绍</h3><p>​    下面介绍的模型训练数据默认为 one-hot 编码。</p><h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>​    通常情况下 RNN 架构的网络不会有很多层(一般不大于3层), 下面以一层举例, 每一层的输入是 a^t 和 x^t 这两个变量,  输出是 a^(t+1) 和 y^t ,  y^t 是由 sigmoid 激活函数激活的, 表示此处为各个 word 的概率, a^(t+1) 一般是由 tanh 激活函数激活的, 作为下一时间步网络的输入与 x^(t+1) 一同输入模型, 依此一直循环。a^(t+1) 和 y^t 在图中给出。</p><p>​    <img src="/images/post_3/1.png" alt="RNN前向传播原理"></p><p>​     注意上面给出的原理图只是为了方便理解每个时间步干了什么, 把循环展开了, 真正的 RNN 结构图其实是循环的。</p><p>​    <img src="/images/post_3/14.png" alt="RNN原理图"></p><p>​     从前向传播原理图中可以得出在训练过程中模型学习的参数是 Wa、Wy、Ba、By。</p><h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>​    了解模型的反向传播可以帮我们更好的理解模型学习的过程, 其实单纯是我把反向传播过程的一些细节给忘了, 想回忆一下。关于反向传播基本概念和原理可以参考一下<a href="https://zhuanlan.zhihu.com/p/261710847">这篇文章</a>。下图是关于 RNN 的反向传播过程。</p><p>​    <img src="/images/post_3/3.png" alt="RNN反向传播过程"></p><p>​     图中的红色箭头表明的反向传播路径。简单来说就是定义一个损失函数(有的文章叫做误差函数)，根据前向传播得到的输出与实际值计算损失函数，再通过链式法则对权重(在这里是Wa\Wy\Ba\By)进行求导。</p><p>​     将原来的权重值 — 求导的结果*lr 得到新的权重完成梯度下降的过程。</p><h3 id="不同输出的-RNN-网络"><a href="#不同输出的-RNN-网络" class="headerlink" title="不同输出的 RNN 网络"></a>不同输出的 RNN 网络</h3><p>​    在RNN应用领域那张图上可以看到 RNN 有很多应用领域, 模型的输入和输出并不一定是严格一对一的，特定的任务情境下 RNN 网络的输出数量并不一定等于输入的数量, 因此要根据特定任务场景调整网络的输出。</p><h4 id="一对多网络"><a href="#一对多网络" class="headerlink" title="一对多网络"></a>一对多网络</h4><p>   一对多常用在生成任务上, 比如音乐的生成, 输入可以为空或者一个表示音乐基调的单词, 输出则为不等长的音符组成音乐，这种网络结构如下图所示。</p><p>​        <img src="/images/post_3/6.png" alt="One2Many"></p><h4 id="多对一网络"><a href="#多对一网络" class="headerlink" title="多对一网络"></a>多对一网络</h4><p>​    多对一网络常用在舆论检测上, 比如饭店评价通过输入一段话来评判客户对这家饭店的打星, 比如打星范围为1-5, 那么模型的唯一输出则为打星值。这种网络结构如下图所示。</p><p>​    <img src="/images/post_3/7.png" alt="Many2One">    </p><h4 id="多对多网络"><a href="#多对多网络" class="headerlink" title="多对多网络"></a>多对多网络</h4><p>​    这种网络最常见, 有时候输入的序列长度跟输出序列长度并不相等, 比如机器翻译应用。这种网络结构如下图所示，我们常将带有 x 输入的模型的前半部分叫做 encoder(编码器), 带有 y 输出的模型后半部分叫做 decoder(译码器)。</p><p>​    <img src="/images/post_3/15.png" alt="Many2Many">        </p><h3 id="双向-RNN-BRNN"><a href="#双向-RNN-BRNN" class="headerlink" title="双向 RNN (BRNN)"></a>双向 RNN (BRNN)</h3><p>   RNN 网络虽然解决了序列输入的训练问题，但是如果碰到类似于下面这种训练输入情况</p><figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk"><span class="hljs-comment">&quot;Hello, Teddy Bear is on sale.&quot;</span><br><span class="hljs-comment">&quot;Hello, Teddy is coming.&quot;</span><br></code></pre></td></tr></table></figure><p>   RNN 在序列预测时, 如果前面的三个词是 “Hello, Teddy”, 这时候模型很难正确预测下面的输出是什么，这也是单向 RNN 网络的局限性，为了解决这种问题，出现了双向循环网络(Bidirectional RNN), 从左向右与从右向左同时开始训练。原理图如下所示。</p><p>​    <img src="/images/post_3/12.png" alt="BRNN原理图">            </p><h3 id="深层-RNN-DRNN"><a href="#深层-RNN-DRNN" class="headerlink" title="深层 RNN (DRNN)"></a>深层 RNN (DRNN)</h3><p>   在构建 RNN 网络时, 因为时序循环的存在使得网络本身就具有了很多层的网络, 因此 RNN 的层数一般没有很多，但当训练数据量增大时，深层 RNN (Deep RNN) 的效果比单层的 RNN 效果往往更好。深层 RNN 的激活向量的运作如下图所示, 就是单纯地给每层都像以前一样重复操作。</p><p>​    <img src="/images/post_3/13.png" alt="DRNN原理图">     </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>   RNN的出现极大推动了NLP领域的发展, 最后用科学家们对其评价来结尾。</p><blockquote><p>“RNN和其变种（如LSTM、GRU）在处理时间序列和序列数据方面具有巨大的潜力，尤其是在语音识别、翻译、语音生成等领域。然而，Transformer模型的出现正在改变这一领域的游戏规则，Transformer在处理长序列数据时展示出了更高的效率和性能。”</p><p>​                                                                                                                                                            ——Andrew Ng</p></blockquote><blockquote><p>“尽管RNN在很多应用中表现出色，但在处理长序列时，尤其是需要捕捉远距离依赖关系的情况下，Transformer等新型架构可能会更加适合。”</p><p>​                                                                                                                                                        ——Ilya Sutskever</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL]Self-attention学习</title>
    <link href="/2024/07/10/DL-Self-attention%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/07/10/DL-Self-attention%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="新的任务场景"><a href="#新的任务场景" class="headerlink" title="新的任务场景"></a>新的任务场景</h4><p>​    在前面的分类模型(如CNN等)的学习中, 模型的输入(如图像)是一个固定大小向量且在批输入时的各个输入向量通常并无直接关系, 有些任务下(如文字处理)需要变长的向量作为模型的输入, 且输入的各个向量之间有一定的语义关系，这时就要考虑改进模型以适应这种任务的需求。</p><p>​    通常情况下这种模型的输出有三种情况(如下图所示):    </p><p><img src="/images/post_2/5.png" alt="模型输出的三种情况"></p><ul><li><p>输入的每个向量都对应一个输出 seqence labeling</p><p>常用于词性标注等任务</p></li><li><p>输入的n个变量只对应一个输出</p><p>常用于舆情分析(从一句话中抽取对某事的态度)、药物毒性检测。</p></li><li><p>输入的n个变量对应m个输出 seq2seq</p><p>应用最广泛，机器翻译，分子预测，对话任务等。</p></li></ul><h4 id="新的问题"><a href="#新的问题" class="headerlink" title="新的问题"></a>新的问题</h4><p>​    对 Sequence Labeling 而言，我们想要n个输入的变长向量对应n个输出，我们自然而然的想到可以给每个向量都加上一个 FC 层，这样可以得到n个输出向量，但是这样就完全把语义信息抛弃了，词与词之间的关系没有用上。像 “I saw a saw”这个例子，saw既有动词”看”的意思, 又有名词”锯子”的意思，但是如果你输入同样的 FC 层，输出肯定只能有一种结果。</p><h3 id="Self-Attention-原理"><a href="#Self-Attention-原理" class="headerlink" title="Self-Attention 原理"></a>Self-Attention 原理</h3><h4 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h4><p>​    简单来说， Self-Attention 就是把各个词的相关度也作为输入输进了模型，如下图</p><p><img src="/images/post_2/4.png" alt="将相关度作为输入"></p><p>​     然后再将输出作为输入进入 FC 层(可以再将FC层的输出作为输入再进self-attention层)</p><p><img src="/images/post_2/1.png" alt="Self-Attention的总览"></p><h4 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h4><p>​    目前主流算相关度的方法是Google提出的 Dot-product 方法，以算 a1 与 a2、 a3、 a4 的相关度为例:</p><ul><li><p>先算a1的q(quary, 查找的意思, 去找 a1 与其他输入的相关度) q &#x3D; Wq*a1 (??? 这里的Wq咋来的，是一个标量还是一个向量，算每个输入的 q 时这个Wq是一样的吗)</p></li><li><p>再算其他输入的k, k &#x3D; Wk * an (??? 这里的Wk也同问)，这里也有必要算一下 a1 的 k</p></li><li><p>将a1的q与每个k作点积运算，得到相关系数a1,1 &#x2F; a1,2 等等</p></li><li><p>将得到的相关系数经过一层softmax(不一定非要softmax, ReLU等也可以)得到a’1,1 &#x2F; a’1,2 等等</p><p> <img src="/images/post_2/2.png" alt="求相关度原理图"></p></li><li><p>算每个输入的 v &#x3D; Wv * a1(??? 这里的Wv也同问)</p></li><li><p>将每个 v与对应的 a’ 相乘再相加得到 b1, 如果 b1 很接近 v 与 a’1,2 的结果，那么就跟输入2更相关</p><p> <img src="/images/post_2/3.png" alt="求b的原理图"></p></li></ul><h4 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h4><ul><li><p>将各个input vector 合成矩阵 I, 李宏毅老师的 ppt 很明了, 竖着放置相当于将各个 vector 竖列组合。再将其分别与Wq、Wk、Wv作矩阵相乘运算, 得到 Q、 K、 V。每列即为算得的qi、ki、vi。</p><p><img src="/images/post_2/6.png" alt="求矩阵QKV"></p></li><li><p>再将 K.T 与 Q 作矩阵乘法, 得到A, 经过softmax得到A’, A’ 的第n行第m列即为 自注意力score a’n,m。</p><p><img src="/images/post_2/7.png" alt="求A&#39;"></p></li><li><p>再将 V 与矩阵 A’ 作矩阵乘法, 得到矩阵 O, 每一列即为 bi。</p><p><img src="/images/post_2/8.png" alt="求A&#39;"></p></li><li><p>上面整个流程如下图所示, 这个流程中所需要学习的参数就是 Wq、Wk、Wv。</p><p><img src="/images/post_2/9.png" alt="self-attention矩阵运算流程"></p></li></ul><h3 id="Self-Attention-vs-CNN"><a href="#Self-Attention-vs-CNN" class="headerlink" title="Self-Attention vs CNN"></a>Self-Attention vs CNN</h3><p>   研究表明, Self-Attention 是一中更加 flexible 的 CNN, 而CNN 则是受限的 Self-Attention。self-attention是可以通过调整某些参数变成CNN的。</p><p><img src="/images/post_2/13.png" alt="Self-Attention vs CNN"></p><h3 id="Muti-head-Self-Attention"><a href="#Muti-head-Self-Attention" class="headerlink" title="Muti-head Self-Attention"></a>Muti-head Self-Attention</h3><p>   关于多头注意力机制，其思路就是将每个q、k、v分成多路，然后照着一样的流程各自去算矩阵O’，最后再将矩阵O’通过运算化成矩阵O。</p><p>   为了方便理解，可以这样想为什么要多头注意力机制: 如果只用一头注意力机制，那么这个q就负责这所有的相关性信息的搜索。如果用多头注意力机制，那么每个q就可以专门负责某一方面的搜寻，使得训练得更全面。</p><p><img src="/images/post_2/10.png" alt="多头注意力机制的原理图"></p><p>​    将q分成多路的运算: [q1,1 , q1,2] &#x3D; W_q_muti x q</p><p>​    K、V分成多路的运算也同理。</p><p>​    这样需要学习的参数又多了 W_q_muti、W_k_muti、W_v_muti、W_o_muti(形状不同前面三个, 作用是将多个O’合成一个O,如图)。</p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>​    上面提到的 Self-Attention 虽然将各个input vector 之间的关系考虑了进来, 但是仍然忽略了各个input vector 的位置关系。比如训练数据为语言的时候，最前面位置的词一般是主语。这时候就要考虑位置的因素了，这时候可以将每个输入向量加上对应的位置向量ei, 再进行self-attention。</p><p><img src="/images/post_2/11.png" alt="Positional Encoding"></p><p>​    目前有种 ei 的实现方法, transformer 那篇论文中提出的 Postional Encoding 是一种手工实现的 ei, 事先将每个位置的 ei 都规定好了。当然还有将 ei 设置为待学习的参数进行学习的方法。</p><p>​     <a href="https://blog.csdn.net/m0_37605642/article/details/132866365#:~:text=%E6%9C%AC%E8%8A%82%E4%BB%A5%20Sinusoidal">transform中的Postional Encoding</a></p><h3 id="Self-Attention的应用"><a href="#Self-Attention的应用" class="headerlink" title="Self-Attention的应用"></a>Self-Attention的应用</h3><h4 id="Self-Attention-for-Speech"><a href="#Self-Attention-for-Speech" class="headerlink" title="Self-Attention for Speech"></a>Self-Attention for Speech</h4><p>​    语音一般是一个很长的序列, 如果将这个序列中的每个vector的关系都算score的话, 计算量会很大, 所以一般会将计算的范围限制在一定长度。</p><p><img src="/images/post_2/12.png" alt="Self-Attention for Speech"></p><h4 id="Self-Attention-for-Image"><a href="#Self-Attention-for-Image" class="headerlink" title="Self-Attention for Image"></a>Self-Attention for Image</h4><p>   图像也可以作为序列做self-attention。一般输入的图像都是RGB通道的，这时每个 pixel 包含的三个值可以作为sequence的一个vector。这时候sequence的长度取决于图像的大小。</p><p><img src="/images/post_2/13.png" alt="Self-Attention for Image"></p><h4 id="Self-Attention-for-Graph"><a href="#Self-Attention-for-Graph" class="headerlink" title="Self-Attention for Graph"></a>Self-Attention for Graph</h4><p>   图也可以做self-attention, 这时候只需要考虑每个结点与之相连结点的score即可。</p><p><img src="/images/post_2/15.png" alt="Self-Attention for Graph"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>nlp</tag>
      
      <tag>注意力机制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>畸形的内卷</title>
    <link href="/2024/07/09/%E7%95%B8%E5%BD%A2%E7%9A%84%E5%86%85%E5%8D%B7/"/>
    <url>/2024/07/09/%E7%95%B8%E5%BD%A2%E7%9A%84%E5%86%85%E5%8D%B7/</url>
    
    <content type="html"><![CDATA[<h3 id="内卷与方向"><a href="#内卷与方向" class="headerlink" title="内卷与方向"></a>内卷与方向</h3><h4 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h4><p>​今天下午在调飞机闲聊时，刚好聊到了分流的事情，学妹让我给些建议(当然其实我觉得我只能给出每个专业的利弊，最后的定夺是应该结合自己的人生目标选择的)。聊着聊着学弟就聊起了他的舍友：</p><blockquote><p>“我舍友上学期几门课卡了3.9的绩点，最后磨老师全给改成了4.1的绩点。”</p></blockquote><p>​我其实挺看不起这种行为的，大家本来都是一起学的，凭什么你能把绩点给改了，这对别人一点也不公平。</p><p>​其实上面的见怪不怪，但是最让我震惊的是学弟学妹竟然也想这样做，讨论让老师给自己改成绩的方法。我心想我天，这都是什么鬼(先叠一下，假如这篇博客不小心被学弟学妹看到了，不要怪我，在这给你们道个歉)。</p><h4 id="看法"><a href="#看法" class="headerlink" title="看法"></a>看法</h4><p>​聊聊我对这些事的看法吧，先声明是拙见。</p><p>​目前高考之前的教育体制让学生们都变得太”唯分数论”了，他们自信地以为只要我的绩点足够高，那么万事万物都是美好的。可是，然后呢？绩点刷到 4.0x 然后呢？</p><p>​你说，然后我就保研了啊。可是，保研了之后呢？再在研究生阶段接着卷绩点吗？</p><blockquote><p>“你始终都在被绩点、综测推着去做事情，你始终不知道你想要的是什么”</p></blockquote><p>​我发现这种现象越来越严重了，大学慢慢的变成了另一所高中。诚实的说，我在大一的时候也是这样的想法:为什么加基地呢？因为可以加综测(歪一下，但是当时面试的时候我还是说的是因为有很多厉害的学长学姐，可以向他们学习，现在看来太难绷了 hhhhh )。</p><p>​虽然之前我也这样想，但是在基地的这一年多，我慢慢的清楚了自己想要的是什么，我会衡量一下为了实现想要的，我要不要去努力的卷绩点、综测，我应该做些什么去实现我想要的，我应该把绩点、综测搞到什么程度就可以了，毕竟这东西在大学非常耗费时间和经历。</p><p>​这样卷搞得大学生活真的没滋没味，太难受了，至少我的前两年是这样觉得的。</p><h4 id="难以破局"><a href="#难以破局" class="headerlink" title="难以破局"></a>难以破局</h4><p>​其实我觉得只要弄清你要什么，你想得到什么，很容易破局。如果你最终是要去公司工作，这时候你就要考虑一下读研对你的工作到底有没有实质性的帮助。其实我感觉三年的工作经验跟读研三年出来工作很难衡量哪个更好。为什么说难以破局，很多人可能跟我一样，想试试自己有没有科研的潜力，再决定自己未来的道路，这时候就要不得不卷一下或者考研了。哎，很难说。</p><p>​但其实我觉得一个人最终能达到什么样的高度，跟自己的性格高度相关，你如果不知道该咋办了，那就听从自己的内心吧(就是做自己觉得应该做的)，每个人都不一样也很难说。</p><h4 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h4><p>​原来这里有很多话想说，但是我都删了。</p><p>​想家了。</p><table><thead><tr><th>觉得赵雷的《<a href="https://music.163.com/#/song?id=447926063">朵</a>》很好听 特别是前奏！</th></tr></thead></table>]]></content>
    
    
    <categories>
      
      <category>感悟</category>
      
    </categories>
    
    
    <tags>
      
      <tag>内卷</tag>
      
      <tag>人生目标</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="/2024/07/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <url>/2024/07/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h3 id="我的第一篇博客"><a href="#我的第一篇博客" class="headerlink" title="我的第一篇博客"></a>我的第一篇博客</h3><h5 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h5><p>​纪念一下自己的第一篇博客。感觉我自己还算是一个喜欢分享的人(maybe)，虽然我一直发私密的动态。暑期在做集训的间隙搭建了这个博客网站，感觉还不错，就当QQ空间&#x2F;朋友圈用了。</p><p>​网站是跟着csdn上的教程搭的，部署在github上，基本上不费工夫，搭搭积木一样。搭好之后对网站进行改造有一种小时候玩4399换装的感觉。</p><h5 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h5><p>​我为什么搭建这个博客？我们从初中就开始学习历史，历史事件的发生一般有三种原因: 根本原因、直接原因、导火索。</p><p>​我觉得我搭这个网站的根本原因就是自己是一个喜欢记录的人，但是又不想让别人看见，所以搭这个博客正好满足了我的需求，不出意外的话，这网站我是不会给朋友们说的。直接原因就是今年无人机视觉太难做了，反复折磨，想着记录一下被折磨的过程。导火索是看到wh学长的博客，很早之前写的博客还有callback，很戳我，我很喜欢这样的记录方式。所以立马就搭了这个博客。</p><h5 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h5><p>​不出意外的话，以后我对生活的一些感悟和经历、一些想法、学习的过程、对技术的理解(虽然也没啥理解)，都会在这里更新，希望我自己能坚持吧。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
